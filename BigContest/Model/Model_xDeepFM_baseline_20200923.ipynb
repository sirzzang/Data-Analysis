{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_xDeepFM_baseline_20200923.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhA7Xh_0Gp8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "outputId": "b8fc49dc-1f45-4a09-a402-af6e667c66fd"
      },
      "source": [
        "!pip install deepctr[gpu]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepctr[gpu]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/5eb51503413cfb057c7f232f0bb77cecda043ae2d9d6868727790cd135db/deepctr-0.8.1-py3-none-any.whl (110kB)\n",
            "\r\u001b[K     |███                             | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 51kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 61kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.23.0)\n",
            "Collecting tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr[gpu]) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr[gpu]) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (3.0.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.35.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (50.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu, deepctr\n",
            "Successfully installed deepctr-0.8.1 tensorflow-gpu-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsajrDJHPX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈 불러오기\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# from deepctr.models import DeepFM\n",
        "from deepctr.models.xdeepfm import xDeepFM\n",
        "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nojY7e7VHR6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 로드\n",
        "# data = pd.read_csv(\"./data/범주데이터모음.csv\", usecols=lambda x: 'Unnamed' not in x)\n",
        "data = pd.read_csv(\"/content/drive/Shared drives/빅콘테스트/데이터(전처리 후 논의 대상)/범주데이터모음2.csv\", \n",
        "                   usecols=lambda x: 'Unnamed' not in x)\n",
        "data2 = pd.read_csv(\"/content/drive/Shared drives/빅콘테스트/데이터(전처리 후 논의 대상)/deepfm데이터.csv\",\n",
        "                    usecols=lambda x: 'Unnamed' not in x)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOBOG-BoILMS",
        "colab_type": "text"
      },
      "source": [
        "# 컬럼 확인\n",
        "\n",
        "* Issue1: 데이터 오류\n",
        "    - `deepfm데이터.csv` 사용하면 판다스 에러가 난다.\n",
        "    - 일단 기존에 사용한 `범주데이터모음2.csv`를 사용해서 판매 단가만 붙인다.\n",
        "    - 취급액이 0인 데이터도 제외한다.\n",
        "\n",
        "* Issue2: 데이터 더 넣어도 될 듯? \n",
        "    - dense features로서 기온, 습도 등 시계열 데이터, \n",
        "    - sparse feature로서 미세먼지 등?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsL9DRWhIM5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "768623b1-ae7f-4bbf-9d38-080913a91a9b"
      },
      "source": [
        "# 컬럼 확인\n",
        "len(data.columns), data.columns, len(data2.columns), data2.columns"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47,\n",
              " Index(['취급액', '요일', '시간', '월', '고유상품명', '계절', '노출횟수', '주의보bin3', '주의보bin4',\n",
              "        '주의보유무', '경보유무', '일별강수량sum_cat', '일별적설sum_cat', '일별기온평균1', '일별기온최대1',\n",
              "        '일별기온최소1', '일별기온평균2', '일별기온최대2', '일별기온최소2', '일교차1', '일별습도평균1',\n",
              "        '일별습도평균2', '일별습도최대2', '일조량1', '요일 맵핑', '시간 맵핑', '월 맵핑', '고유상품명 맵핑',\n",
              "        '계절 맵핑', '노출횟수 맵핑', '주의보bin3 맵핑', '주의보bin4 맵핑', '주의보유무 맵핑', '경보유무 맵핑',\n",
              "        '일별강수량sum_cat 맵핑', '일별적설sum_cat 맵핑', '일별기온평균1 맵핑', '일별기온최대1 맵핑',\n",
              "        '일별기온최소1 맵핑', '일별기온평균2 맵핑', '일별기온최대2 맵핑', '일별기온최소2 맵핑', '일교차1 맵핑',\n",
              "        '일별습도평균1 맵핑', '일별습도평균2 맵핑', '일별습도최대2 맵핑', '일조량1 맵핑'],\n",
              "       dtype='object'),\n",
              " 27,\n",
              " Index(['취급액', '요일 맵핑', '시간 맵핑', '월 맵핑', '성별 맵핑', '고유상품명 맵핑', '브랜드 맵핑', '계절 맵핑',\n",
              "        '노출횟수 맵핑', '주의보bin3 맵핑', '주의보bin4 맵핑', '주의보유무 맵핑', '경보유무 맵핑',\n",
              "        '일별강수량sum_cat 맵핑', '일별적설sum_cat 맵핑', '일별기온평균1 맵핑', '일별기온최대1 맵핑',\n",
              "        '일별기온최소1 맵핑', '일별기온평균2 맵핑', '일별기온최대2 맵핑', '일별기온최소2 맵핑', '일교차1 맵핑',\n",
              "        '일별습도평균1 맵핑', '일별습도평균2 맵핑', '일별습도최대2 맵핑', '일조량1 맵핑', '판매단가'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxuqwdnQU8sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "05de9ff5-05b6-42e7-e1ca-7485bd07d7fc"
      },
      "source": [
        "# 데이터 추출\n",
        "df1 = data[[x for x in data.columns if '맵핑' in x or x == '취급액']]\n",
        "df2 = data2[['판매단가']]\n",
        "df = pd.concat([df1, df2], axis=1)\n",
        "df.tail()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>취급액</th>\n",
              "      <th>요일 맵핑</th>\n",
              "      <th>시간 맵핑</th>\n",
              "      <th>월 맵핑</th>\n",
              "      <th>고유상품명 맵핑</th>\n",
              "      <th>계절 맵핑</th>\n",
              "      <th>노출횟수 맵핑</th>\n",
              "      <th>주의보bin3 맵핑</th>\n",
              "      <th>주의보bin4 맵핑</th>\n",
              "      <th>주의보유무 맵핑</th>\n",
              "      <th>경보유무 맵핑</th>\n",
              "      <th>일별강수량sum_cat 맵핑</th>\n",
              "      <th>일별적설sum_cat 맵핑</th>\n",
              "      <th>일별기온평균1 맵핑</th>\n",
              "      <th>일별기온최대1 맵핑</th>\n",
              "      <th>일별기온최소1 맵핑</th>\n",
              "      <th>일별기온평균2 맵핑</th>\n",
              "      <th>일별기온최대2 맵핑</th>\n",
              "      <th>일별기온최소2 맵핑</th>\n",
              "      <th>일교차1 맵핑</th>\n",
              "      <th>일별습도평균1 맵핑</th>\n",
              "      <th>일별습도평균2 맵핑</th>\n",
              "      <th>일별습도최대2 맵핑</th>\n",
              "      <th>일조량1 맵핑</th>\n",
              "      <th>판매단가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13274</th>\n",
              "      <td>76448000</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13275</th>\n",
              "      <td>58429000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>148000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13276</th>\n",
              "      <td>128803000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13277</th>\n",
              "      <td>16713000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13278</th>\n",
              "      <td>64651000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             취급액  요일 맵핑  시간 맵핑  월 맵핑  ...  일별습도평균2 맵핑  일별습도최대2 맵핑  일조량1 맵핑     판매단가\n",
              "13274   76448000      0     15    11  ...           0           0        0  1499000\n",
              "13275   58429000      0     17    11  ...           0           0        0   148000\n",
              "13276  128803000      0     17    11  ...           0           0        0   168000\n",
              "13277   16713000      0     17    11  ...           0           0        0   158000\n",
              "13278   64651000      0     17    11  ...           0           0        0   178000\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPZMTHQLVVGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "44d507d6-40df-446a-bb6e-1613e3c6836f"
      },
      "source": [
        "# 취급액 0인 데이터 제외\n",
        "df = df.loc[df['취급액'] != 0].reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>취급액</th>\n",
              "      <th>요일 맵핑</th>\n",
              "      <th>시간 맵핑</th>\n",
              "      <th>월 맵핑</th>\n",
              "      <th>고유상품명 맵핑</th>\n",
              "      <th>계절 맵핑</th>\n",
              "      <th>노출횟수 맵핑</th>\n",
              "      <th>주의보bin3 맵핑</th>\n",
              "      <th>주의보bin4 맵핑</th>\n",
              "      <th>주의보유무 맵핑</th>\n",
              "      <th>경보유무 맵핑</th>\n",
              "      <th>일별강수량sum_cat 맵핑</th>\n",
              "      <th>일별적설sum_cat 맵핑</th>\n",
              "      <th>일별기온평균1 맵핑</th>\n",
              "      <th>일별기온최대1 맵핑</th>\n",
              "      <th>일별기온최소1 맵핑</th>\n",
              "      <th>일별기온평균2 맵핑</th>\n",
              "      <th>일별기온최대2 맵핑</th>\n",
              "      <th>일별기온최소2 맵핑</th>\n",
              "      <th>일교차1 맵핑</th>\n",
              "      <th>일별습도평균1 맵핑</th>\n",
              "      <th>일별습도평균2 맵핑</th>\n",
              "      <th>일별습도최대2 맵핑</th>\n",
              "      <th>일조량1 맵핑</th>\n",
              "      <th>판매단가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12033000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20663000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47878000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99736000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90973000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>79000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13004</th>\n",
              "      <td>76448000</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13005</th>\n",
              "      <td>58429000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>148000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13006</th>\n",
              "      <td>128803000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13007</th>\n",
              "      <td>16713000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13008</th>\n",
              "      <td>64651000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>178000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13009 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             취급액  요일 맵핑  시간 맵핑  월 맵핑  ...  일별습도평균2 맵핑  일별습도최대2 맵핑  일조량1 맵핑     판매단가\n",
              "0       12033000      0      0     0  ...           0           0        0    39900\n",
              "1       20663000      0      0     0  ...           0           0        0    39900\n",
              "2       47878000      0      1     0  ...           0           0        0    59000\n",
              "3       99736000      0      2     0  ...           0           0        0    59900\n",
              "4       90973000      0      3     0  ...           0           0        0    79000\n",
              "...          ...    ...    ...   ...  ...         ...         ...      ...      ...\n",
              "13004   76448000      0     15    11  ...           0           0        0  1499000\n",
              "13005   58429000      0     17    11  ...           0           0        0   148000\n",
              "13006  128803000      0     17    11  ...           0           0        0   168000\n",
              "13007   16713000      0     17    11  ...           0           0        0   158000\n",
              "13008   64651000      0     17    11  ...           0           0        0   178000\n",
              "\n",
              "[13009 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkUJxW4FE-fA",
        "colab_type": "text"
      },
      "source": [
        "# 모델링\n",
        "\n",
        " price scaling 방법\n",
        "- `np.log1p`\n",
        "- `minmax scaling`\n",
        "- `np.log1p` + `minmax scaling`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erw_l1d9VcXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "05d7ba48-4a18-4554-d76b-1aa09c9f8564"
      },
      "source": [
        "# 컬럼 이름 영어로 변환\n",
        "df.columns = ['AMT', 'day', 'hour', 'month', 'product_name', 'season', 'exposure_cnt',\n",
        "              'warning_bin3', 'warning_bin4', 'is_warning', 'is_alert', 'daily_precipitation_sum', \n",
        "              'daily_snow_sum', 'daily_temp_mean_1', 'daily_temp_max_1', 'daily_temp_min_1',\n",
        "              'daily_temp_mean_2', 'daily_temp_max_2', 'daily_temp_min_2', 'daily_temp_diff_1', 'daily_humidity_mean_1', \n",
        "              'daily_humidity_mean_2', 'daily_humidity_max_2', 'daily_sunshine',\n",
        "              'product_price']\n",
        "df.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AMT</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>product_name</th>\n",
              "      <th>season</th>\n",
              "      <th>exposure_cnt</th>\n",
              "      <th>warning_bin3</th>\n",
              "      <th>warning_bin4</th>\n",
              "      <th>is_warning</th>\n",
              "      <th>is_alert</th>\n",
              "      <th>daily_precipitation_sum</th>\n",
              "      <th>daily_snow_sum</th>\n",
              "      <th>daily_temp_mean_1</th>\n",
              "      <th>daily_temp_max_1</th>\n",
              "      <th>daily_temp_min_1</th>\n",
              "      <th>daily_temp_mean_2</th>\n",
              "      <th>daily_temp_max_2</th>\n",
              "      <th>daily_temp_min_2</th>\n",
              "      <th>daily_temp_diff_1</th>\n",
              "      <th>daily_humidity_mean_1</th>\n",
              "      <th>daily_humidity_mean_2</th>\n",
              "      <th>daily_humidity_max_2</th>\n",
              "      <th>daily_sunshine</th>\n",
              "      <th>product_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12033000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20663000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47878000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99736000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90973000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>79000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        AMT  day  hour  ...  daily_humidity_max_2  daily_sunshine  product_price\n",
              "0  12033000    0     0  ...                     0               0          39900\n",
              "1  20663000    0     0  ...                     0               0          39900\n",
              "2  47878000    0     1  ...                     0               0          59000\n",
              "3  99736000    0     2  ...                     0               0          59900\n",
              "4  90973000    0     3  ...                     0               0          79000\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDWcd7gmXeyQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ddc806d0-ae8a-4a87-a69c-37c46de47661"
      },
      "source": [
        "# price scaling\n",
        "df['product_price'] = np.log1p(df['product_price'])\n",
        "df.tail()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AMT</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>product_name</th>\n",
              "      <th>season</th>\n",
              "      <th>exposure_cnt</th>\n",
              "      <th>warning_bin3</th>\n",
              "      <th>warning_bin4</th>\n",
              "      <th>is_warning</th>\n",
              "      <th>is_alert</th>\n",
              "      <th>daily_precipitation_sum</th>\n",
              "      <th>daily_snow_sum</th>\n",
              "      <th>daily_temp_mean_1</th>\n",
              "      <th>daily_temp_max_1</th>\n",
              "      <th>daily_temp_min_1</th>\n",
              "      <th>daily_temp_mean_2</th>\n",
              "      <th>daily_temp_max_2</th>\n",
              "      <th>daily_temp_min_2</th>\n",
              "      <th>daily_temp_diff_1</th>\n",
              "      <th>daily_humidity_mean_1</th>\n",
              "      <th>daily_humidity_mean_2</th>\n",
              "      <th>daily_humidity_max_2</th>\n",
              "      <th>daily_sunshine</th>\n",
              "      <th>product_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13004</th>\n",
              "      <td>76448000</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14.220309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13005</th>\n",
              "      <td>58429000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11.904974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13006</th>\n",
              "      <td>128803000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12.031725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13007</th>\n",
              "      <td>16713000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11.970357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13008</th>\n",
              "      <td>64651000</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12.089544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             AMT  day  ...  daily_sunshine  product_price\n",
              "13004   76448000    0  ...               0      14.220309\n",
              "13005   58429000    0  ...               0      11.904974\n",
              "13006  128803000    0  ...               0      12.031725\n",
              "13007   16713000    0  ...               0      11.970357\n",
              "13008   64651000    0  ...               0      12.089544\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pD-PlzuFpyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "edbfa920-40b2-4d4c-8839-25197d89ff66"
      },
      "source": [
        "# feature, target 정의\n",
        "sparse_features = ['day', 'hour', 'month', 'product_name', 'season', 'exposure_cnt',\n",
        "                   'warning_bin3', 'warning_bin4', 'is_warning', 'is_alert', 'daily_precipitation_sum', \n",
        "                   'daily_snow_sum', 'daily_temp_mean_1', 'daily_temp_max_1', 'daily_temp_min_1',\n",
        "                   'daily_temp_mean_2', 'daily_temp_max_2', 'daily_temp_min_2', 'daily_temp_diff_1', 'daily_humidity_mean_1',\n",
        "                   'daily_humidity_mean_2', 'daily_humidity_max_2', 'daily_sunshine']\n",
        "dense_features = ['product_price']\n",
        "target = ['AMT']\n",
        "\n",
        "# feature column 정의\n",
        "fixlen_feature_columns = [SparseFeat(feat, df[feat].nunique(), embedding_dim=6) for feat in sparse_features]\\\n",
        "                        + [DenseFeat(feat, 1, ) for feat in dense_features]                                                         \n",
        "linear_feature_columns = fixlen_feature_columns\n",
        "dnn_feature_columns = fixlen_feature_columns\n",
        "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
        "print(fixlen_feature_columns)\n",
        "print(feature_names)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SparseFeat(name='day', vocabulary_size=7, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e3284e0f0>, embedding_name='day', group_name='default_group', trainable=True), SparseFeat(name='hour', vocabulary_size=21, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e3284e240>, embedding_name='hour', group_name='default_group', trainable=True), SparseFeat(name='month', vocabulary_size=12, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efde80>, embedding_name='month', group_name='default_group', trainable=True), SparseFeat(name='product_name', vocabulary_size=956, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdef0>, embedding_name='product_name', group_name='default_group', trainable=True), SparseFeat(name='season', vocabulary_size=4, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdf28>, embedding_name='season', group_name='default_group', trainable=True), SparseFeat(name='exposure_cnt', vocabulary_size=80, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdf60>, embedding_name='exposure_cnt', group_name='default_group', trainable=True), SparseFeat(name='warning_bin3', vocabulary_size=3, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdf98>, embedding_name='warning_bin3', group_name='default_group', trainable=True), SparseFeat(name='warning_bin4', vocabulary_size=4, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdfd0>, embedding_name='warning_bin4', group_name='default_group', trainable=True), SparseFeat(name='is_warning', vocabulary_size=2, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31efdda0>, embedding_name='is_warning', group_name='default_group', trainable=True), SparseFeat(name='is_alert', vocabulary_size=1, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d048>, embedding_name='is_alert', group_name='default_group', trainable=True), SparseFeat(name='daily_precipitation_sum', vocabulary_size=6, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d0b8>, embedding_name='daily_precipitation_sum', group_name='default_group', trainable=True), SparseFeat(name='daily_snow_sum', vocabulary_size=2, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d0f0>, embedding_name='daily_snow_sum', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_mean_1', vocabulary_size=20, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d128>, embedding_name='daily_temp_mean_1', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_max_1', vocabulary_size=20, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d160>, embedding_name='daily_temp_max_1', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_min_1', vocabulary_size=20, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d198>, embedding_name='daily_temp_min_1', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_mean_2', vocabulary_size=12, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d1d0>, embedding_name='daily_temp_mean_2', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_max_2', vocabulary_size=12, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d208>, embedding_name='daily_temp_max_2', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_min_2', vocabulary_size=12, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d240>, embedding_name='daily_temp_min_2', group_name='default_group', trainable=True), SparseFeat(name='daily_temp_diff_1', vocabulary_size=8, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d278>, embedding_name='daily_temp_diff_1', group_name='default_group', trainable=True), SparseFeat(name='daily_humidity_mean_1', vocabulary_size=16, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d2b0>, embedding_name='daily_humidity_mean_1', group_name='default_group', trainable=True), SparseFeat(name='daily_humidity_mean_2', vocabulary_size=10, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d2e8>, embedding_name='daily_humidity_mean_2', group_name='default_group', trainable=True), SparseFeat(name='daily_humidity_max_2', vocabulary_size=10, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d320>, embedding_name='daily_humidity_max_2', group_name='default_group', trainable=True), SparseFeat(name='daily_sunshine', vocabulary_size=7, embedding_dim=6, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f4e31f0d358>, embedding_name='daily_sunshine', group_name='default_group', trainable=True), DenseFeat(name='product_price', dimension=1, dtype='float32')]\n",
            "['day', 'hour', 'month', 'product_name', 'season', 'exposure_cnt', 'warning_bin3', 'warning_bin4', 'is_warning', 'is_alert', 'daily_precipitation_sum', 'daily_snow_sum', 'daily_temp_mean_1', 'daily_temp_max_1', 'daily_temp_min_1', 'daily_temp_mean_2', 'daily_temp_max_2', 'daily_temp_min_2', 'daily_temp_diff_1', 'daily_humidity_mean_1', 'daily_humidity_mean_2', 'daily_humidity_max_2', 'daily_sunshine', 'product_price']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56X79-YTFtCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 트레인, 테스트 셋 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df[target],\n",
        "                                                    test_size=0.2, random_state=2020)\n",
        "\n",
        "# 입력 데이터\n",
        "X_train = {name:X_train[name].values for name in feature_names}\n",
        "X_test = {name:X_test[name].values for name in feature_names}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfGnTSGVL1hH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d1e4caf-73cb-48e7-9608-f0182ff08d7f"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "# 모델 학습\n",
        "model = xDeepFM(linear_feature_columns, dnn_feature_columns, \n",
        "                dnn_use_bn=True, task='regression')\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001), loss='mape')\n",
        "# model.compile(optimizers.RMSprop(learning_rate=0.001), loss='mape')\n",
        "\n",
        "# 모델 훈련\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 batch_size=256,\n",
        "                 epochs=500,\n",
        "                 verbose=1,\n",
        "                 validation_split=0.2)\n",
        "\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.plot(hist.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Model Train-Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 2s 48ms/step - loss: 100.0000 - val_loss: 100.0000\n",
            "Epoch 2/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 100.0000 - val_loss: 100.0000\n",
            "Epoch 3/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9999 - val_loss: 99.9999\n",
            "Epoch 4/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9998 - val_loss: 99.9999\n",
            "Epoch 5/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9998 - val_loss: 99.9998\n",
            "Epoch 6/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9996 - val_loss: 99.9998\n",
            "Epoch 7/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9994 - val_loss: 99.9996\n",
            "Epoch 8/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9991 - val_loss: 99.9995\n",
            "Epoch 9/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9988 - val_loss: 99.9994\n",
            "Epoch 10/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9985 - val_loss: 99.9992\n",
            "Epoch 11/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9982 - val_loss: 99.9990\n",
            "Epoch 12/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9979 - val_loss: 99.9984\n",
            "Epoch 13/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9974 - val_loss: 99.9983\n",
            "Epoch 14/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 99.9963 - val_loss: 99.9966\n",
            "Epoch 15/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 99.9906 - val_loss: 99.9810\n",
            "Epoch 16/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 99.9125 - val_loss: 99.7181\n",
            "Epoch 17/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 98.6096 - val_loss: 96.1785\n",
            "Epoch 18/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 90.9853 - val_loss: 84.7319\n",
            "Epoch 19/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 81.2048 - val_loss: 77.8317\n",
            "Epoch 20/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 78.4964 - val_loss: 76.2290\n",
            "Epoch 21/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 77.1646 - val_loss: 74.9803\n",
            "Epoch 22/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 75.9591 - val_loss: 73.8769\n",
            "Epoch 23/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 74.7218 - val_loss: 72.6253\n",
            "Epoch 24/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 73.4822 - val_loss: 71.2687\n",
            "Epoch 25/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 72.0742 - val_loss: 69.8660\n",
            "Epoch 26/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 70.6163 - val_loss: 68.3446\n",
            "Epoch 27/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 69.0359 - val_loss: 66.7600\n",
            "Epoch 28/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 67.4173 - val_loss: 65.0950\n",
            "Epoch 29/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 65.7430 - val_loss: 63.3466\n",
            "Epoch 30/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 63.9006 - val_loss: 61.5512\n",
            "Epoch 31/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 62.1326 - val_loss: 59.8757\n",
            "Epoch 32/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 60.3968 - val_loss: 58.3173\n",
            "Epoch 33/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 58.8226 - val_loss: 56.9674\n",
            "Epoch 34/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 57.4018 - val_loss: 55.7224\n",
            "Epoch 35/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 56.0570 - val_loss: 54.6227\n",
            "Epoch 36/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 54.7296 - val_loss: 53.6005\n",
            "Epoch 37/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 53.4779 - val_loss: 52.5156\n",
            "Epoch 38/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 52.2148 - val_loss: 51.5244\n",
            "Epoch 39/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 51.0072 - val_loss: 50.7172\n",
            "Epoch 40/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 49.9114 - val_loss: 49.8045\n",
            "Epoch 41/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 48.8746 - val_loss: 49.0154\n",
            "Epoch 42/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 47.9284 - val_loss: 48.3535\n",
            "Epoch 43/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 47.0986 - val_loss: 47.7165\n",
            "Epoch 44/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 46.2887 - val_loss: 47.2082\n",
            "Epoch 45/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 45.5693 - val_loss: 46.6990\n",
            "Epoch 46/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 44.9220 - val_loss: 46.2871\n",
            "Epoch 47/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 44.2900 - val_loss: 45.9202\n",
            "Epoch 48/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 43.7876 - val_loss: 45.6581\n",
            "Epoch 49/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 43.2407 - val_loss: 45.2618\n",
            "Epoch 50/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 42.7968 - val_loss: 45.0910\n",
            "Epoch 51/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 42.3858 - val_loss: 44.8334\n",
            "Epoch 52/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 41.9553 - val_loss: 44.5932\n",
            "Epoch 53/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 41.5516 - val_loss: 44.3576\n",
            "Epoch 54/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 41.2525 - val_loss: 44.1827\n",
            "Epoch 55/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 40.9318 - val_loss: 44.0563\n",
            "Epoch 56/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 40.5599 - val_loss: 43.8181\n",
            "Epoch 57/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 40.3083 - val_loss: 43.7574\n",
            "Epoch 58/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 40.0031 - val_loss: 43.6189\n",
            "Epoch 59/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 39.7313 - val_loss: 43.4748\n",
            "Epoch 60/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 39.5158 - val_loss: 43.4509\n",
            "Epoch 61/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 39.2981 - val_loss: 43.2282\n",
            "Epoch 62/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 39.0673 - val_loss: 43.1452\n",
            "Epoch 63/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 38.8541 - val_loss: 43.0061\n",
            "Epoch 64/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 38.6504 - val_loss: 42.9128\n",
            "Epoch 65/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 38.4903 - val_loss: 42.8160\n",
            "Epoch 66/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 38.3107 - val_loss: 42.8404\n",
            "Epoch 67/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 38.2160 - val_loss: 42.6964\n",
            "Epoch 68/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 38.0234 - val_loss: 42.5788\n",
            "Epoch 69/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 37.8650 - val_loss: 42.5337\n",
            "Epoch 70/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 37.7174 - val_loss: 42.4188\n",
            "Epoch 71/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 37.5901 - val_loss: 42.3515\n",
            "Epoch 72/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 37.4649 - val_loss: 42.2815\n",
            "Epoch 73/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 37.3593 - val_loss: 42.2007\n",
            "Epoch 74/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 37.2293 - val_loss: 42.1501\n",
            "Epoch 75/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 37.1317 - val_loss: 42.0956\n",
            "Epoch 76/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 37.0004 - val_loss: 42.1015\n",
            "Epoch 77/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 36.9196 - val_loss: 41.9569\n",
            "Epoch 78/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 36.8120 - val_loss: 41.9325\n",
            "Epoch 79/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 36.7209 - val_loss: 41.8983\n",
            "Epoch 80/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 36.6405 - val_loss: 41.8320\n",
            "Epoch 81/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 36.5603 - val_loss: 41.7973\n",
            "Epoch 82/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 36.4784 - val_loss: 41.7576\n",
            "Epoch 83/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 36.4290 - val_loss: 41.6243\n",
            "Epoch 84/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 36.3067 - val_loss: 41.6447\n",
            "Epoch 85/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 36.2523 - val_loss: 41.5494\n",
            "Epoch 86/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 36.1663 - val_loss: 41.5226\n",
            "Epoch 87/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 36.0915 - val_loss: 41.4622\n",
            "Epoch 88/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 36.0272 - val_loss: 41.4930\n",
            "Epoch 89/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.9494 - val_loss: 41.3673\n",
            "Epoch 90/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 35.8915 - val_loss: 41.3172\n",
            "Epoch 91/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.8344 - val_loss: 41.3154\n",
            "Epoch 92/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 35.7706 - val_loss: 41.2068\n",
            "Epoch 93/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.7184 - val_loss: 41.2666\n",
            "Epoch 94/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.6433 - val_loss: 41.2013\n",
            "Epoch 95/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.5986 - val_loss: 41.0549\n",
            "Epoch 96/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.5778 - val_loss: 41.0820\n",
            "Epoch 97/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.4771 - val_loss: 40.9691\n",
            "Epoch 98/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.4666 - val_loss: 41.0250\n",
            "Epoch 99/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.3895 - val_loss: 40.9028\n",
            "Epoch 100/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.3362 - val_loss: 40.9144\n",
            "Epoch 101/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.2791 - val_loss: 40.8758\n",
            "Epoch 102/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.2355 - val_loss: 40.8812\n",
            "Epoch 103/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.2069 - val_loss: 40.7515\n",
            "Epoch 104/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.1716 - val_loss: 40.7021\n",
            "Epoch 105/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 35.1117 - val_loss: 40.7376\n",
            "Epoch 106/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 35.0585 - val_loss: 40.6002\n",
            "Epoch 107/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 35.0465 - val_loss: 40.7057\n",
            "Epoch 108/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 35.0129 - val_loss: 40.6347\n",
            "Epoch 109/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.9246 - val_loss: 40.5933\n",
            "Epoch 110/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.9255 - val_loss: 40.5203\n",
            "Epoch 111/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.8685 - val_loss: 40.4839\n",
            "Epoch 112/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.8274 - val_loss: 40.4756\n",
            "Epoch 113/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 34.8052 - val_loss: 40.4581\n",
            "Epoch 114/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.7549 - val_loss: 40.4196\n",
            "Epoch 115/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.7137 - val_loss: 40.3659\n",
            "Epoch 116/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.7113 - val_loss: 40.3019\n",
            "Epoch 117/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 34.6716 - val_loss: 40.3512\n",
            "Epoch 118/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 34.6543 - val_loss: 40.2667\n",
            "Epoch 119/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.6167 - val_loss: 40.2978\n",
            "Epoch 120/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 34.5595 - val_loss: 40.2674\n",
            "Epoch 121/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.5485 - val_loss: 40.2044\n",
            "Epoch 122/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.5204 - val_loss: 40.2318\n",
            "Epoch 123/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.5030 - val_loss: 40.1381\n",
            "Epoch 124/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 34.4700 - val_loss: 40.1832\n",
            "Epoch 125/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.4303 - val_loss: 40.1288\n",
            "Epoch 126/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.3884 - val_loss: 40.0636\n",
            "Epoch 127/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.3665 - val_loss: 40.0073\n",
            "Epoch 128/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.3336 - val_loss: 39.9945\n",
            "Epoch 129/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.3267 - val_loss: 39.9750\n",
            "Epoch 130/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.2943 - val_loss: 39.9965\n",
            "Epoch 131/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.2899 - val_loss: 39.8908\n",
            "Epoch 132/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.2688 - val_loss: 39.9435\n",
            "Epoch 133/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.2175 - val_loss: 39.9419\n",
            "Epoch 134/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.1835 - val_loss: 39.8557\n",
            "Epoch 135/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.1834 - val_loss: 39.8362\n",
            "Epoch 136/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.1615 - val_loss: 39.8120\n",
            "Epoch 137/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.1203 - val_loss: 39.8182\n",
            "Epoch 138/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.1060 - val_loss: 39.7682\n",
            "Epoch 139/500\n",
            "33/33 [==============================] - 1s 23ms/step - loss: 34.0772 - val_loss: 39.7262\n",
            "Epoch 140/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.0594 - val_loss: 39.7191\n",
            "Epoch 141/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 34.0518 - val_loss: 39.7886\n",
            "Epoch 142/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 34.0264 - val_loss: 39.7337\n",
            "Epoch 143/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 34.0032 - val_loss: 39.6537\n",
            "Epoch 144/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.9989 - val_loss: 39.6878\n",
            "Epoch 145/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.9513 - val_loss: 39.6542\n",
            "Epoch 146/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.9452 - val_loss: 39.6539\n",
            "Epoch 147/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.9300 - val_loss: 39.5712\n",
            "Epoch 148/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.9013 - val_loss: 39.6523\n",
            "Epoch 149/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.9074 - val_loss: 39.5499\n",
            "Epoch 150/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8908 - val_loss: 39.5771\n",
            "Epoch 151/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8665 - val_loss: 39.6430\n",
            "Epoch 152/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8840 - val_loss: 39.4854\n",
            "Epoch 153/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8636 - val_loss: 39.5338\n",
            "Epoch 154/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8390 - val_loss: 39.4623\n",
            "Epoch 155/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8116 - val_loss: 39.4716\n",
            "Epoch 156/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.7814 - val_loss: 39.4341\n",
            "Epoch 157/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.8226 - val_loss: 39.5081\n",
            "Epoch 158/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.7694 - val_loss: 39.4729\n",
            "Epoch 159/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.7435 - val_loss: 39.4529\n",
            "Epoch 160/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.7547 - val_loss: 39.4057\n",
            "Epoch 161/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.7187 - val_loss: 39.4229\n",
            "Epoch 162/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.7218 - val_loss: 39.3870\n",
            "Epoch 163/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.7106 - val_loss: 39.3760\n",
            "Epoch 164/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.6579 - val_loss: 39.3840\n",
            "Epoch 165/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.6570 - val_loss: 39.3757\n",
            "Epoch 166/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.6212 - val_loss: 39.3647\n",
            "Epoch 167/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.6095 - val_loss: 39.3647\n",
            "Epoch 168/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.6087 - val_loss: 39.3057\n",
            "Epoch 169/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.5845 - val_loss: 39.3900\n",
            "Epoch 170/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.6221 - val_loss: 39.3283\n",
            "Epoch 171/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.5651 - val_loss: 39.3301\n",
            "Epoch 172/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.5376 - val_loss: 39.2955\n",
            "Epoch 173/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.5254 - val_loss: 39.3252\n",
            "Epoch 174/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.5542 - val_loss: 39.2599\n",
            "Epoch 175/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.5276 - val_loss: 39.3173\n",
            "Epoch 176/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.4949 - val_loss: 39.2769\n",
            "Epoch 177/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.4841 - val_loss: 39.2927\n",
            "Epoch 178/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.4607 - val_loss: 39.2713\n",
            "Epoch 179/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.4551 - val_loss: 39.2942\n",
            "Epoch 180/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.4418 - val_loss: 39.2250\n",
            "Epoch 181/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.4219 - val_loss: 39.2281\n",
            "Epoch 182/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.4278 - val_loss: 39.2701\n",
            "Epoch 183/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.4096 - val_loss: 39.1601\n",
            "Epoch 184/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.4020 - val_loss: 39.2725\n",
            "Epoch 185/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.3771 - val_loss: 39.2156\n",
            "Epoch 186/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.3643 - val_loss: 39.1507\n",
            "Epoch 187/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.3457 - val_loss: 39.1576\n",
            "Epoch 188/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.3417 - val_loss: 39.2060\n",
            "Epoch 189/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.3090 - val_loss: 39.1288\n",
            "Epoch 190/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.3296 - val_loss: 39.3040\n",
            "Epoch 191/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.3447 - val_loss: 39.1582\n",
            "Epoch 192/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.3176 - val_loss: 39.1454\n",
            "Epoch 193/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.2797 - val_loss: 39.0720\n",
            "Epoch 194/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.2609 - val_loss: 39.1575\n",
            "Epoch 195/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.2482 - val_loss: 39.1002\n",
            "Epoch 196/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.2428 - val_loss: 39.1012\n",
            "Epoch 197/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.2291 - val_loss: 39.1315\n",
            "Epoch 198/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.2068 - val_loss: 39.1351\n",
            "Epoch 199/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1915 - val_loss: 39.1397\n",
            "Epoch 200/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1937 - val_loss: 39.0877\n",
            "Epoch 201/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.1826 - val_loss: 39.0812\n",
            "Epoch 202/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1530 - val_loss: 39.0978\n",
            "Epoch 203/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.1351 - val_loss: 39.0778\n",
            "Epoch 204/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.1295 - val_loss: 39.0351\n",
            "Epoch 205/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1109 - val_loss: 39.1766\n",
            "Epoch 206/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1280 - val_loss: 38.9998\n",
            "Epoch 207/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.1197 - val_loss: 39.0695\n",
            "Epoch 208/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.1180 - val_loss: 39.0862\n",
            "Epoch 209/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.0477 - val_loss: 38.9699\n",
            "Epoch 210/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.0442 - val_loss: 39.0097\n",
            "Epoch 211/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 33.0481 - val_loss: 39.0564\n",
            "Epoch 212/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 33.0098 - val_loss: 38.9746\n",
            "Epoch 213/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 33.0448 - val_loss: 39.0407\n",
            "Epoch 214/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.9974 - val_loss: 39.0255\n",
            "Epoch 215/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.9787 - val_loss: 38.9916\n",
            "Epoch 216/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.9552 - val_loss: 39.0980\n",
            "Epoch 217/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.9485 - val_loss: 38.9912\n",
            "Epoch 218/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.9422 - val_loss: 39.0249\n",
            "Epoch 219/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.9251 - val_loss: 39.0960\n",
            "Epoch 220/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.8922 - val_loss: 39.1077\n",
            "Epoch 221/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.8681 - val_loss: 39.0190\n",
            "Epoch 222/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.8738 - val_loss: 39.1297\n",
            "Epoch 223/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.8503 - val_loss: 38.9842\n",
            "Epoch 224/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 32.8540 - val_loss: 39.0495\n",
            "Epoch 225/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.7925 - val_loss: 39.0086\n",
            "Epoch 226/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.8086 - val_loss: 39.1024\n",
            "Epoch 227/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.7968 - val_loss: 39.0453\n",
            "Epoch 228/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.7573 - val_loss: 38.9782\n",
            "Epoch 229/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.7596 - val_loss: 39.1703\n",
            "Epoch 230/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.7507 - val_loss: 39.0360\n",
            "Epoch 231/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.7272 - val_loss: 39.0232\n",
            "Epoch 232/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.6915 - val_loss: 38.9708\n",
            "Epoch 233/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.6616 - val_loss: 39.0141\n",
            "Epoch 234/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.6472 - val_loss: 39.0261\n",
            "Epoch 235/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.6259 - val_loss: 39.0737\n",
            "Epoch 236/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.6036 - val_loss: 39.0616\n",
            "Epoch 237/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.5943 - val_loss: 38.9828\n",
            "Epoch 238/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.5505 - val_loss: 39.0707\n",
            "Epoch 239/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.5539 - val_loss: 39.0655\n",
            "Epoch 240/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.5269 - val_loss: 39.0609\n",
            "Epoch 241/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.5166 - val_loss: 39.1232\n",
            "Epoch 242/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.5078 - val_loss: 39.0060\n",
            "Epoch 243/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.4928 - val_loss: 39.0296\n",
            "Epoch 244/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.4570 - val_loss: 39.0681\n",
            "Epoch 245/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.4330 - val_loss: 38.9894\n",
            "Epoch 246/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.3964 - val_loss: 39.0954\n",
            "Epoch 247/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.3773 - val_loss: 39.0470\n",
            "Epoch 248/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.3610 - val_loss: 39.0271\n",
            "Epoch 249/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.3706 - val_loss: 39.1313\n",
            "Epoch 250/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.3276 - val_loss: 39.0805\n",
            "Epoch 251/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.2884 - val_loss: 39.0175\n",
            "Epoch 252/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.3014 - val_loss: 39.1164\n",
            "Epoch 253/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.2647 - val_loss: 39.0546\n",
            "Epoch 254/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.2329 - val_loss: 39.0036\n",
            "Epoch 255/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.2285 - val_loss: 39.0292\n",
            "Epoch 256/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.2293 - val_loss: 39.2054\n",
            "Epoch 257/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.1864 - val_loss: 39.0830\n",
            "Epoch 258/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.1610 - val_loss: 39.0787\n",
            "Epoch 259/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.1032 - val_loss: 39.0673\n",
            "Epoch 260/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.0980 - val_loss: 39.0425\n",
            "Epoch 261/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 32.0998 - val_loss: 39.0589\n",
            "Epoch 262/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.0690 - val_loss: 39.0583\n",
            "Epoch 263/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 32.0450 - val_loss: 39.1631\n",
            "Epoch 264/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 32.0098 - val_loss: 39.0982\n",
            "Epoch 265/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.9785 - val_loss: 39.1754\n",
            "Epoch 266/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.9659 - val_loss: 39.1970\n",
            "Epoch 267/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.9392 - val_loss: 39.1359\n",
            "Epoch 268/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.9116 - val_loss: 39.0748\n",
            "Epoch 269/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.8976 - val_loss: 39.0479\n",
            "Epoch 270/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.8700 - val_loss: 39.0889\n",
            "Epoch 271/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.8387 - val_loss: 39.1483\n",
            "Epoch 272/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.8196 - val_loss: 39.1899\n",
            "Epoch 273/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.8014 - val_loss: 39.0979\n",
            "Epoch 274/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.7914 - val_loss: 39.2598\n",
            "Epoch 275/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.7649 - val_loss: 39.0805\n",
            "Epoch 276/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.7413 - val_loss: 39.1466\n",
            "Epoch 277/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.6839 - val_loss: 39.1277\n",
            "Epoch 278/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.6622 - val_loss: 39.0927\n",
            "Epoch 279/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.6193 - val_loss: 39.0647\n",
            "Epoch 280/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.6309 - val_loss: 39.0710\n",
            "Epoch 281/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.5925 - val_loss: 39.0752\n",
            "Epoch 282/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.5620 - val_loss: 39.0925\n",
            "Epoch 283/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.5470 - val_loss: 39.1793\n",
            "Epoch 284/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.5207 - val_loss: 39.0906\n",
            "Epoch 285/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.5155 - val_loss: 39.1271\n",
            "Epoch 286/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.4684 - val_loss: 39.1147\n",
            "Epoch 287/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.4676 - val_loss: 39.0952\n",
            "Epoch 288/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.4206 - val_loss: 39.1515\n",
            "Epoch 289/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.4000 - val_loss: 39.1768\n",
            "Epoch 290/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.3500 - val_loss: 39.1553\n",
            "Epoch 291/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.3374 - val_loss: 39.1402\n",
            "Epoch 292/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.3116 - val_loss: 39.1881\n",
            "Epoch 293/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.2826 - val_loss: 39.1874\n",
            "Epoch 294/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.2604 - val_loss: 39.2576\n",
            "Epoch 295/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 31.2818 - val_loss: 39.1755\n",
            "Epoch 296/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.2124 - val_loss: 39.2196\n",
            "Epoch 297/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.1708 - val_loss: 39.2578\n",
            "Epoch 298/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.1582 - val_loss: 39.1552\n",
            "Epoch 299/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.1710 - val_loss: 39.2125\n",
            "Epoch 300/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.1044 - val_loss: 39.1991\n",
            "Epoch 301/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.0790 - val_loss: 39.2493\n",
            "Epoch 302/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.0436 - val_loss: 39.1816\n",
            "Epoch 303/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 31.0163 - val_loss: 39.2339\n",
            "Epoch 304/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.9923 - val_loss: 39.1819\n",
            "Epoch 305/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 31.0092 - val_loss: 39.2120\n",
            "Epoch 306/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.9398 - val_loss: 39.3675\n",
            "Epoch 307/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.9371 - val_loss: 39.1749\n",
            "Epoch 308/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.9015 - val_loss: 39.2071\n",
            "Epoch 309/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.8849 - val_loss: 39.2891\n",
            "Epoch 310/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.8754 - val_loss: 39.2258\n",
            "Epoch 311/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.8279 - val_loss: 39.2468\n",
            "Epoch 312/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.8009 - val_loss: 39.2461\n",
            "Epoch 313/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.7686 - val_loss: 39.2466\n",
            "Epoch 314/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.7814 - val_loss: 39.2025\n",
            "Epoch 315/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.7194 - val_loss: 39.2525\n",
            "Epoch 316/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.6803 - val_loss: 39.2667\n",
            "Epoch 317/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.6577 - val_loss: 39.2744\n",
            "Epoch 318/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.6371 - val_loss: 39.2646\n",
            "Epoch 319/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.6141 - val_loss: 39.3723\n",
            "Epoch 320/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.6309 - val_loss: 39.2120\n",
            "Epoch 321/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.5904 - val_loss: 39.2626\n",
            "Epoch 322/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.5445 - val_loss: 39.3198\n",
            "Epoch 323/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.5146 - val_loss: 39.3219\n",
            "Epoch 324/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.4958 - val_loss: 39.2898\n",
            "Epoch 325/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.4791 - val_loss: 39.3842\n",
            "Epoch 326/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.4536 - val_loss: 39.2321\n",
            "Epoch 327/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.4161 - val_loss: 39.3048\n",
            "Epoch 328/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.3852 - val_loss: 39.3137\n",
            "Epoch 329/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.3679 - val_loss: 39.2963\n",
            "Epoch 330/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.3469 - val_loss: 39.3871\n",
            "Epoch 331/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.3262 - val_loss: 39.3506\n",
            "Epoch 332/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.2866 - val_loss: 39.3451\n",
            "Epoch 333/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.2960 - val_loss: 39.4049\n",
            "Epoch 334/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.2483 - val_loss: 39.3657\n",
            "Epoch 335/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.2694 - val_loss: 39.2611\n",
            "Epoch 336/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.2006 - val_loss: 39.2942\n",
            "Epoch 337/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.1944 - val_loss: 39.3443\n",
            "Epoch 338/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.1682 - val_loss: 39.2691\n",
            "Epoch 339/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.1179 - val_loss: 39.2867\n",
            "Epoch 340/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.0837 - val_loss: 39.4069\n",
            "Epoch 341/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 30.0963 - val_loss: 39.3838\n",
            "Epoch 342/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 30.0398 - val_loss: 39.3143\n",
            "Epoch 343/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 30.0219 - val_loss: 39.3750\n",
            "Epoch 344/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.9996 - val_loss: 39.3658\n",
            "Epoch 345/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.9781 - val_loss: 39.3087\n",
            "Epoch 346/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.9518 - val_loss: 39.3303\n",
            "Epoch 347/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.9305 - val_loss: 39.4107\n",
            "Epoch 348/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.9057 - val_loss: 39.3592\n",
            "Epoch 349/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.9344 - val_loss: 39.3319\n",
            "Epoch 350/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.8714 - val_loss: 39.3696\n",
            "Epoch 351/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.8350 - val_loss: 39.3976\n",
            "Epoch 352/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.7974 - val_loss: 39.3567\n",
            "Epoch 353/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.7810 - val_loss: 39.3631\n",
            "Epoch 354/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.7467 - val_loss: 39.3635\n",
            "Epoch 355/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.7194 - val_loss: 39.4008\n",
            "Epoch 356/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.6998 - val_loss: 39.3555\n",
            "Epoch 357/500\n",
            "33/33 [==============================] - 1s 22ms/step - loss: 29.6874 - val_loss: 39.3871\n",
            "Epoch 358/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.6696 - val_loss: 39.3952\n",
            "Epoch 359/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.6534 - val_loss: 39.3966\n",
            "Epoch 360/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.6076 - val_loss: 39.4453\n",
            "Epoch 361/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.5884 - val_loss: 39.4476\n",
            "Epoch 362/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.5596 - val_loss: 39.3606\n",
            "Epoch 363/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.5385 - val_loss: 39.4162\n",
            "Epoch 364/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.5218 - val_loss: 39.4649\n",
            "Epoch 365/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.5385 - val_loss: 39.5960\n",
            "Epoch 366/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.5259 - val_loss: 39.4182\n",
            "Epoch 367/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.4613 - val_loss: 39.4113\n",
            "Epoch 368/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.4443 - val_loss: 39.4596\n",
            "Epoch 369/500\n",
            "33/33 [==============================] - 1s 18ms/step - loss: 29.4387 - val_loss: 39.4691\n",
            "Epoch 370/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.4169 - val_loss: 39.3948\n",
            "Epoch 371/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.3821 - val_loss: 39.5373\n",
            "Epoch 372/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.3914 - val_loss: 39.4437\n",
            "Epoch 373/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.3606 - val_loss: 39.4655\n",
            "Epoch 374/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.3251 - val_loss: 39.4511\n",
            "Epoch 375/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.2885 - val_loss: 39.5463\n",
            "Epoch 376/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.2702 - val_loss: 39.4771\n",
            "Epoch 377/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 29.2452 - val_loss: 39.4759\n",
            "Epoch 378/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.2134 - val_loss: 39.4953\n",
            "Epoch 379/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.2025 - val_loss: 39.5040\n",
            "Epoch 380/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.1718 - val_loss: 39.5260\n",
            "Epoch 381/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.1474 - val_loss: 39.5542\n",
            "Epoch 382/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.1492 - val_loss: 39.4944\n",
            "Epoch 383/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.1186 - val_loss: 39.4586\n",
            "Epoch 384/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.1658 - val_loss: 39.5045\n",
            "Epoch 385/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.0800 - val_loss: 39.5404\n",
            "Epoch 386/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.0539 - val_loss: 39.5089\n",
            "Epoch 387/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.0938 - val_loss: 39.4678\n",
            "Epoch 388/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.0775 - val_loss: 39.5034\n",
            "Epoch 389/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 29.0121 - val_loss: 39.6044\n",
            "Epoch 390/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 29.0015 - val_loss: 39.5315\n",
            "Epoch 391/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.9680 - val_loss: 39.5514\n",
            "Epoch 392/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.9162 - val_loss: 39.5640\n",
            "Epoch 393/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.9349 - val_loss: 39.6159\n",
            "Epoch 394/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.9118 - val_loss: 39.6736\n",
            "Epoch 395/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.8746 - val_loss: 39.5653\n",
            "Epoch 396/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.8485 - val_loss: 39.5874\n",
            "Epoch 397/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.8300 - val_loss: 39.6261\n",
            "Epoch 398/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.8449 - val_loss: 39.6009\n",
            "Epoch 399/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.7998 - val_loss: 39.6093\n",
            "Epoch 400/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.7768 - val_loss: 39.7638\n",
            "Epoch 401/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.7795 - val_loss: 39.5912\n",
            "Epoch 402/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.7251 - val_loss: 39.6500\n",
            "Epoch 403/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.7077 - val_loss: 39.6519\n",
            "Epoch 404/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.6823 - val_loss: 39.6455\n",
            "Epoch 405/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.6716 - val_loss: 39.6002\n",
            "Epoch 406/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.6288 - val_loss: 39.6499\n",
            "Epoch 407/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.6418 - val_loss: 39.6122\n",
            "Epoch 408/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.6314 - val_loss: 39.6658\n",
            "Epoch 409/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.5901 - val_loss: 39.6912\n",
            "Epoch 410/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.5551 - val_loss: 39.6100\n",
            "Epoch 411/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.5403 - val_loss: 39.5887\n",
            "Epoch 412/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.5253 - val_loss: 39.6944\n",
            "Epoch 413/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.4944 - val_loss: 39.7372\n",
            "Epoch 414/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.4794 - val_loss: 39.6974\n",
            "Epoch 415/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.4715 - val_loss: 39.6294\n",
            "Epoch 416/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.4383 - val_loss: 39.6481\n",
            "Epoch 417/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.4235 - val_loss: 39.7213\n",
            "Epoch 418/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.4188 - val_loss: 39.7178\n",
            "Epoch 419/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.3747 - val_loss: 39.7750\n",
            "Epoch 420/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.3503 - val_loss: 39.7160\n",
            "Epoch 421/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.3797 - val_loss: 39.7600\n",
            "Epoch 422/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.3269 - val_loss: 39.7530\n",
            "Epoch 423/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.3360 - val_loss: 39.8319\n",
            "Epoch 424/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.2888 - val_loss: 39.7849\n",
            "Epoch 425/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.2800 - val_loss: 39.7809\n",
            "Epoch 426/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.2444 - val_loss: 39.7578\n",
            "Epoch 427/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.2385 - val_loss: 39.7897\n",
            "Epoch 428/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.2576 - val_loss: 39.7800\n",
            "Epoch 429/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.2198 - val_loss: 39.7480\n",
            "Epoch 430/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.1826 - val_loss: 39.7735\n",
            "Epoch 431/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.1461 - val_loss: 39.7827\n",
            "Epoch 432/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.1334 - val_loss: 39.7679\n",
            "Epoch 433/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.1254 - val_loss: 39.7583\n",
            "Epoch 434/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.1177 - val_loss: 39.8315\n",
            "Epoch 435/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.0924 - val_loss: 39.8153\n",
            "Epoch 436/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.0839 - val_loss: 39.8602\n",
            "Epoch 437/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.0627 - val_loss: 39.9461\n",
            "Epoch 438/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 28.0523 - val_loss: 39.8422\n",
            "Epoch 439/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 28.0393 - val_loss: 39.9090\n",
            "Epoch 440/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 28.0449 - val_loss: 39.9031\n",
            "Epoch 441/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.9972 - val_loss: 39.7920\n",
            "Epoch 442/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.9573 - val_loss: 39.8711\n",
            "Epoch 443/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.9382 - val_loss: 39.8326\n",
            "Epoch 444/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.9243 - val_loss: 39.7875\n",
            "Epoch 445/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.9117 - val_loss: 39.8498\n",
            "Epoch 446/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.8783 - val_loss: 39.9117\n",
            "Epoch 447/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.8805 - val_loss: 39.8556\n",
            "Epoch 448/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.8955 - val_loss: 39.9571\n",
            "Epoch 449/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.9127 - val_loss: 39.9232\n",
            "Epoch 450/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.8549 - val_loss: 39.8772\n",
            "Epoch 451/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.8583 - val_loss: 40.0765\n",
            "Epoch 452/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.8127 - val_loss: 39.9498\n",
            "Epoch 453/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.7850 - val_loss: 39.8667\n",
            "Epoch 454/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.7315 - val_loss: 40.0162\n",
            "Epoch 455/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.7595 - val_loss: 39.9859\n",
            "Epoch 456/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.7573 - val_loss: 39.9021\n",
            "Epoch 457/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.7352 - val_loss: 40.0154\n",
            "Epoch 458/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.6866 - val_loss: 39.9570\n",
            "Epoch 459/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.6971 - val_loss: 40.0050\n",
            "Epoch 460/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.6606 - val_loss: 39.9978\n",
            "Epoch 461/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.6639 - val_loss: 39.9622\n",
            "Epoch 462/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.6318 - val_loss: 39.9507\n",
            "Epoch 463/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.6428 - val_loss: 39.9779\n",
            "Epoch 464/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.6399 - val_loss: 39.9087\n",
            "Epoch 465/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.6177 - val_loss: 40.0496\n",
            "Epoch 466/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.5823 - val_loss: 39.9777\n",
            "Epoch 467/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.5686 - val_loss: 39.9968\n",
            "Epoch 468/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.5518 - val_loss: 39.9325\n",
            "Epoch 469/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.5439 - val_loss: 40.0869\n",
            "Epoch 470/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.5408 - val_loss: 40.0656\n",
            "Epoch 471/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.5231 - val_loss: 40.0321\n",
            "Epoch 472/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.5190 - val_loss: 39.9768\n",
            "Epoch 473/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.4881 - val_loss: 40.0877\n",
            "Epoch 474/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.4712 - val_loss: 39.9941\n",
            "Epoch 475/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.4675 - val_loss: 40.0067\n",
            "Epoch 476/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.4988 - val_loss: 40.0689\n",
            "Epoch 477/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.4398 - val_loss: 39.9906\n",
            "Epoch 478/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.4129 - val_loss: 40.0535\n",
            "Epoch 479/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.4095 - val_loss: 40.0263\n",
            "Epoch 480/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.3879 - val_loss: 39.9962\n",
            "Epoch 481/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.3810 - val_loss: 40.0904\n",
            "Epoch 482/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.3580 - val_loss: 40.0444\n",
            "Epoch 483/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.3512 - val_loss: 40.0602\n",
            "Epoch 484/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.3261 - val_loss: 40.0620\n",
            "Epoch 485/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.3273 - val_loss: 40.0860\n",
            "Epoch 486/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.2915 - val_loss: 40.0896\n",
            "Epoch 487/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.2953 - val_loss: 40.0115\n",
            "Epoch 488/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.2650 - val_loss: 40.0429\n",
            "Epoch 489/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.2704 - val_loss: 40.0917\n",
            "Epoch 490/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.2485 - val_loss: 40.1106\n",
            "Epoch 491/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.2372 - val_loss: 40.1086\n",
            "Epoch 492/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.2050 - val_loss: 40.1251\n",
            "Epoch 493/500\n",
            "33/33 [==============================] - 1s 20ms/step - loss: 27.2155 - val_loss: 40.1367\n",
            "Epoch 494/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.1890 - val_loss: 40.1358\n",
            "Epoch 495/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.1841 - val_loss: 40.1667\n",
            "Epoch 496/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.1622 - val_loss: 40.1851\n",
            "Epoch 497/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.1669 - val_loss: 40.1187\n",
            "Epoch 498/500\n",
            "33/33 [==============================] - 1s 19ms/step - loss: 27.1631 - val_loss: 40.1628\n",
            "Epoch 499/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.1438 - val_loss: 40.1221\n",
            "Epoch 500/500\n",
            "33/33 [==============================] - 1s 21ms/step - loss: 27.1315 - val_loss: 40.2075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILiHB3NYZH2a",
        "colab_type": "text"
      },
      "source": [
        "# 예측\n",
        "\n",
        " Train, Validation Loss는 괜찮은데 test data에 대해서는 별로 좋지 않은 듯하다. 튜닝 필요함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT0a-rtCaL2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 평가지표 산식\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xwbvb9IZQPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "cbbdfcaa-2d83-4b98-c206-39e1bb050f67"
      },
      "source": [
        "# 예측\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.9476310e+06],\n",
              "       [4.0036056e+07],\n",
              "       [1.5206029e+08],\n",
              "       ...,\n",
              "       [8.2791105e+06],\n",
              "       [3.8618904e+07],\n",
              "       [8.6990704e+07]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdzpgjVEaKHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5110a91-e60b-4425-f696-d2b600848d74"
      },
      "source": [
        "# MAPE 계산\n",
        "mean_absolute_percentage_error(y_pred.reshape(-1, ), y_test['AMT'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149.51093239423034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}