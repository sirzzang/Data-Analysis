{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_DeepFM_test_sourcecode_20200920",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y8WpISi1GGP",
        "colab_type": "text"
      },
      "source": [
        "# DeepFFM 모델링\n",
        "\n",
        "FFM 모델(논문: https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)을 DeepLearning을 활용하여 빌드(논문: https://arxiv.org/abs/1703.04247)하기 위해 소스 코드(출처: https://github.com/shenweichen/DeepCTR)를 뜯어 보자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXlk3QiQY4Cx",
        "colab_type": "text"
      },
      "source": [
        "# *전체 구조*\n",
        "\n",
        "- input_features를 논문 구조에 맞게 맞춰서 구현해 주고,\n",
        "- 케라스 레이어를 상속해서 논문을 구현할 수 있도록 커스텀한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kMowpwHM23d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 파라미터 설정\n",
        "DEFAULT_GROUP_NAME = \"default_group\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNXNPjCum8a1",
        "colab_type": "text"
      },
      "source": [
        "## **1**. utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAl70IYcy39I",
        "colab_type": "text"
      },
      "source": [
        "### 계산\n",
        "\n",
        " 코드 활용에 필요한 계산(?)들이 정의되어 있다. ~~풀링 모드에 따라서 reduce_sum, reduce_max, reduce_mean 다시 정의한 건가? 왜 굳이 tf 내장 함수 안 쓰고?~~ \n",
        "\n",
        "* ~~reduce_sum~~\n",
        "* ~~reduce_max~~ \n",
        "* ~~div~~\n",
        "* ~~softmax~~\n",
        "* ~~reduce_mean~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fssv6HJzm9KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_mean(input_tensor, axis=axis, keep_dims=keep_dims, name=name, reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_mean(input_tensor, axis=axis, keepdims=keep_dims, name=name)\n",
        "\n",
        "def reduce_sum(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keep_dims=keep_dims, name=name, reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_sum(input_tensor, axis=axis, keepdims=keep_dims, name=name)\n",
        "\n",
        "def reduce_max(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None):\n",
        "    try:\n",
        "        return tf.reduce_max(input_tensor, axis=axis, keep_dims=keep_dims, name=name, reduction_indices=reduction_indices)\n",
        "    except TypeError:\n",
        "        return tf.reduce_max(input_tensor, axis=axis, keepdims=keep_dims, name=name)\n",
        "\n",
        "def div(x, y, name=None):\n",
        "    try:\n",
        "        return tf.div(x, y, name=name)\n",
        "    except AttributeError:\n",
        "        return tf.divide(x, y, name=name)\n",
        "\n",
        "def softmax(logits, dim=-1, name=None):\n",
        "    try:\n",
        "        return tf.nn.softmax(logits, dim=dim, name=name)\n",
        "    except TypeError:\n",
        "        return tf.nn.softmax(logits, axis=dim, name=name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DrwR-ROyB9f",
        "colab_type": "text"
      },
      "source": [
        "### 레이어 concat, add\n",
        "\n",
        "- concat_func\n",
        "- add_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBm1y9sByRmn",
        "colab_type": "text"
      },
      "source": [
        "#### concat, add를 위한 클래스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGfA_fxbyG0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer, add\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Add(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Add, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Add, self).build(input_shape) # call해야 함.\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if not isinstance(inputs, list): # 리스트 형태가 아니라면,\n",
        "            return inputs\n",
        "\n",
        "        if len(inputs) == 1:\n",
        "            return inputs[0]\n",
        "        if len(inputs) == 0: # 없으면,\n",
        "            return tf.constant([[0.0]])\n",
        "\n",
        "        return add(inputs)\n",
        "\n",
        "\n",
        "class NoMask(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NoMask, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(NoMask, self).build(input_shape) # call해야 함.\n",
        "\n",
        "    def call(self, x, mask=None, **kwargs):\n",
        "        return x\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHtdbEv_ygUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def add_func(inputs):\n",
        "    return Add()(inputs) # 위에서 정의한 Add 클래스\n",
        "\n",
        "def concat_func(inputs, axis=-1, mask=False):\n",
        "    if not mask:\n",
        "        inputs = list(map(NoMask(), inputs))\n",
        "    if len(inputs) == 1:\n",
        "        return inputs[0]\n",
        "    else:\n",
        "        return Concatenate(axis=axis)(inputs)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0KGHXJy8_M",
        "colab_type": "text"
      },
      "source": [
        "### combined_dnn_input\n",
        "\n",
        " dnn input을 combine한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALIpi7K1zF-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "\n",
        "def combined_dnn_input(sparse_embedding_list, dense_value_list):\n",
        "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
        "        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))\n",
        "        dense_dnn_input = Flatten()(concat_func(dense_value_list))\n",
        "        return concat_func([sparse_dnn_input, dense_dnn_input])\n",
        "    elif len(sparse_embedding_list) > 0:\n",
        "        return Flatten()(concat_func(sparse_embedding_list))\n",
        "    elif len(dense_value_list) > 0:\n",
        "        return Flatten()(concat_func(dense_value_list))\n",
        "    else:\n",
        "        raise NotImplementedError(\"dnn_feature_columns가 비어 있으면 안 됨.\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBm0_U9ikJpI",
        "colab_type": "text"
      },
      "source": [
        "## **2**. Layers\n",
        "\n",
        " 커스텀한 케라스 레이어 클래스들의 모음.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaawiVIMkc13",
        "colab_type": "text"
      },
      "source": [
        "### SequencePoolingLayer\n",
        "\n",
        "> 원래 layers.sequence에 있음.\n",
        "\n",
        " Keras layer를 상속받는다. 가변 길이의 sequence나 multi-value 피쳐에 대해 sum, mean, max 풀링 연산을 한다. \n",
        " \n",
        "1. Input: [seq_value, seq_len] : sequence의 value와 길이를 나타내는 ??두 텐서의 리스트\n",
        "    - seq_value : `(batch_size, T, embedding_size)`\n",
        "    - seq_len : `(batch_size, 1)`\n",
        "\n",
        "2. Output: `(batch_size, 1, embedding_size)`의 3D tensor\n",
        "    - 풀링연산 한 뒤 3차원으로 바꾼 것인가??\n",
        "\n",
        "3. Arguments\n",
        "    - mode: 풀링 연산 종류(mean, max, sum)\n",
        "    - supports_masking: `True`일 경우, 마스킹 가능해야 함.\n",
        "\n",
        "<br>\n",
        "\n",
        " masking 가능 여부에 따라서 달라진다. masking support할 거면 직접 mask 만들어주고 차원 확장해주고, 그게 아니면 인자로 받은 것에서 tensorflow 내장 함수 이용해서 sequence_mask 만들고 축만 바꿔주면 되는 듯. tile 함수 통해서 마스킹하는 과정은 기존에 살펴 본 마스킹과 동일. 0에 가까운 수를 곱해서 빼준다.\n",
        "\n",
        "<br>\n",
        "\n",
        " 축 확장해서 3차원으로!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2kJd-jQkOsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class SeuqencePoolingLayer(Layer):\n",
        "\n",
        "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
        "\n",
        "        if mode not in ['sum', 'mean', 'max']:\n",
        "            raise ValueError(\"풀링 연산은 sum, mean, max 중 하나여야 함.\")\n",
        "\n",
        "        self.mode = mode\n",
        "        self.eps = tf.constant(1e-8, dtype=tf.float32)\n",
        "        self.supports_masking = supports_masking\n",
        "        super(SequencePoolingLayer, self).__init__(**kwargs)     \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        if not self.supports_masking:\n",
        "            self.seq_len_max = int(input_shape[0][1])\n",
        "        super(SequencePoolingLayer, self).build(input_shape) # call해야 함에 주의.\n",
        "    \n",
        "    def call(self, seq_value_len_list, mask=None ,**kwargs):\n",
        "        if self.supports_masking:\n",
        "            if mask is None:\n",
        "                raise ValueError(\"supports_masking 옵션이 True일 때는 input이 masking가능해야 함.\")\n",
        "            uiseq_embed_list = seq_value_len_list\n",
        "            mask = tf.cast(mask, dtype=tf.float32) # tf.to_float(mask)\n",
        "            use_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)\n",
        "            mask = tf.expand_dims(mask, axis=2)\n",
        "        else:\n",
        "            uiseq_embed_list, user_behavior_length = seq_value_len_list\n",
        "            mask = tf.sequence_mask(user_behavior_length, self.seq_len_max, dtype=tf.float32)\n",
        "        \n",
        "        embedding_size = uiseq_embed_list.shape[-1]\n",
        "        mask = tf.tile(mask, [1, 1, embedding_size])\n",
        "\n",
        "        if self.mode == 'max':\n",
        "            hist = uiseq_embed_list - (1-mask) * 1e9\n",
        "            return reduce_max(hist, 1, keep_dims=True)\n",
        "        \n",
        "        if self.mode == 'mean':\n",
        "            hist = div(hist, tf.cast(user_behavior_length, dtype=tf.foat32) + self.eps)\n",
        "        \n",
        "        hist = tf.expand_dims(hist, axis=1)\n",
        "        return hist\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.supports_masking:\n",
        "            return (None, 1, input_shape[-1])\n",
        "        else:\n",
        "            return (None, 1, input_shape[0][-1])\n",
        "    \n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "    \n",
        "    def get_config(self, ):\n",
        "        config = {'mode': self.mode,\n",
        "                  'supports_masking': self.supports_masking}\n",
        "        base_config = super(SequencePoolingLayer, self).get_config()\n",
        "        return dict(list(base_config.items())) + list(config.items())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FxHjt0OqdDj",
        "colab_type": "text"
      },
      "source": [
        "### WeightedSequenceLayer\n",
        "\n",
        "> 원래 layers.sequence에 있음.\n",
        "\n",
        "\n",
        " Keras layer를 상속받는다. 가변 길이의 sequence나 multi-value 피쳐에 대해 weight score를 적용한다.\n",
        " \n",
        "1. Input: [seq_value, seq_len, seq_weight] : sequence의 value와 길이, weight를 나타내는 세 텐서의 리스트.\n",
        "    - seq_value: `(batch_size, T, embedding_size)`\n",
        "    - seq_len: `(batch_size, 1)`\n",
        "    - seq_weight: `(batch_size, T, 1)`\n",
        "\n",
        "2. Output: `(batch_size, 1, embedding_size)`의 3D tensor\n",
        "    - 풀링연산 한 뒤 3차원으로 바꾼 것인가??\n",
        "\n",
        "3. Arguments\n",
        "    - weight_normalization: weight 적용 전에 normalize할 것인지 여부.\n",
        "    - supports_masking: `True`일 경우, 마스킹 가능해야 함.\n",
        "\n",
        "<br>\n",
        "\n",
        " masking 가능 여부, weight_normalization 여부에 따라서 달라진다. weight_normalization은 softmax. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g6ES1vKqc7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class WeightedSequenceLayer(Layer):\n",
        "\n",
        "    def __init__(self, weight_normalization=True, supports_masking=False, **kwargs):\n",
        "        super(WeightedSequenceLayer, self).__init__(**kwargs)\n",
        "        self.weight_normalization = weight_normalization\n",
        "        self.supports_masking = supports_masking\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.supports_masking:\n",
        "            self.seq_len_max = int(input_shape[0][1])\n",
        "        super(WeightedSequenceLayer, self).build(input_shape) # call해야 함에 주의.\n",
        "\n",
        "    def call(self, input_list, mask=None, **kwargs):\n",
        "        if self.supports_masking:\n",
        "            if mask is None:\n",
        "                raise ValueError(\"supports_masking 옵션이 True일 때는 input이 masking가능해야 함.\")\n",
        "            key_input, value_input = input_list\n",
        "            mask = tf.expand_dims(mask[0], axis=2)\n",
        "        else:\n",
        "            key_input, key_length_input, value_input = input_list\n",
        "            mask = tf.sequence_mask(key_length_input, self.seq_len_max, dtype=tf.bool)\n",
        "            mask = tf.transpose(mask, (0, 2, 1))\n",
        "\n",
        "        embedding_size = key_input.shape[-1]\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            paddings = tf.ones_like(value_input) * (-2 ** 32 + 1)\n",
        "        else:\n",
        "            paddings = tf.zeros_like(value_input)\n",
        "        value_input = tf.where(mask, value_input, paddings)\n",
        "\n",
        "        if self.weight_normalization:\n",
        "            value_input = softmax(value_input, dim=1)\n",
        "\n",
        "        if len(value_input.shape) == 2:\n",
        "            value_input = tf.expand_dims(value_input, axis=2)\n",
        "            value_input = tf.tile(value_input, [1, 1, embedding_size])\n",
        "\n",
        "        return tf.multiply(key_input, value_input)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]\n",
        "\n",
        "    def compute_mask(self, inputs, mask):\n",
        "        if self.supports_masking:\n",
        "            return mask[0]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'weight_normalization': self.weight_normalization, \n",
        "                  'supports_masking': self.supports_masking}\n",
        "        base_config = super(WeightedSequenceLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj1RWaD_mIA_",
        "colab_type": "text"
      },
      "source": [
        "### Linear\n",
        "> 원래 utils.py에 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljEZQo_umMC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.initializers import glorot_normal, Zeros\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, l2_reg=0.0, mode=0, use_bias=False, seed=1024, **kwargs):\n",
        "        self.l2_reg = l2_reg\n",
        "        if mode not in [0, 1, 2]:\n",
        "            raise ValueError('mode는 0, 1, 2 중 하나여야 함.')\n",
        "        self.mode = mode\n",
        "        self.use_bias = use_bias\n",
        "        self.seed = seed\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='linear_bias', \n",
        "                                        shape=(1, ), \n",
        "                                        initializer=Zeros(),\n",
        "                                        trainable=True)\n",
        "        if self.mode == 1:\n",
        "            self.kernel = self.add_weight(name='linear_kernel',\n",
        "                                          shape=[int(input_shape[-1]), 1],\n",
        "                                          initializer=glorot_normal(self.seed),\n",
        "                                          regularizer=l2(self.l2_reg),\n",
        "                                          trainable=False)\n",
        "        elif self.mode == 2:\n",
        "            self.kernel = self.add_weight(name='linear_kernel',\n",
        "                                          shape=[int(input_shape[1][-1]), 1],\n",
        "                                          initializer=glorot_normal(self.seed),\n",
        "                                          regularizer=l2(self.l2_reg),\n",
        "                                          trainable=False)\n",
        "        \n",
        "        super(Linear, self).build(input_shape) # 나중에 call해야 함.\n",
        "    \n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.mode == 0:\n",
        "            sparse_input = inputs\n",
        "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)\n",
        "        elif self.mode == 1:\n",
        "            dense_input = inputs\n",
        "            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0)) # dot연산\n",
        "            linear_logit = fc\n",
        "        else:\n",
        "            sparse_input, dense_input = inputs\n",
        "            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))\n",
        "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + fc\n",
        "        if self.use_bias:\n",
        "            linear_logit += self.bias\n",
        "        \n",
        "        return linear_logit\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)\n",
        "    \n",
        "    def compute_mask(self, inputs, mask):\n",
        "        return None\n",
        "    \n",
        "    def get_config(self, ):\n",
        "        config = {'mode': self.mode,\n",
        "                  'l2_reg': self.l2_reg,\n",
        "                  'use_bias': self.use_bias}\n",
        "        base_config = super(Linear, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0HShVZozjPh",
        "colab_type": "text"
      },
      "source": [
        "###  FM\n",
        "\n",
        "> 원래 layers.interaction에 있음.\n",
        "\n",
        " Factorization Machine Layer를 구현한다. feature 간 상호작용 계산 위해 cross term 계산. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TobYhkqezlHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class FM(Layer):\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super(FM, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 3:\n",
        "            raise ValueError(\"차원이 맞지 않음: %d, \\\n",
        "                              3차원이어야 함\" % (len(input_shape)))\n",
        "        super(FM, self).build(input_shape) # call해야 함.\n",
        "    \n",
        "    def call(self, inputs, **kwargs):\n",
        "        if K.ndim(inputs) != 3:\n",
        "            raise ValueError(\"차원이 맞지 않음: %d, \\\n",
        "                              3차원이어야 함\" % (K.ndim(inputs)))\n",
        "        \n",
        "        concated_embeds_value = inputs\n",
        "\n",
        "        square_of_sum = tf.square(reduce_sum(\n",
        "            concated_embeds_value, axis=1, keep_dims=True))\n",
        "        sum_of_square = reduce_sum(\n",
        "            concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n",
        "        cross_term = square_of_sum - sum_of_square\n",
        "        cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=True)\n",
        "\n",
        "        return cross_term\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPyk7ItaSB_e",
        "colab_type": "text"
      },
      "source": [
        "### DNN\n",
        "\n",
        "> 원래 layers.core에 있음.\n",
        "\n",
        " Keras Layer를 상속받아 MLP 레이어 커스텀.\n",
        "\n",
        " 1. Input: `n-dim tensor`. Input으로 들어가는 텐서(shape: `(batch_size, input_dim)`. \n",
        "\n",
        "2. Output: `n-dim tensor`. Output으로 나오는 텐서(shape: `(batch_size, hidden_size[-1])`\n",
        "\n",
        "3. Arguments\n",
        "    - hidden_units\n",
        "    - activation\n",
        "    - l2_reg\n",
        "    - dropout_rate\n",
        "    - use_bn : 배치 노멀라이제이션 여부\n",
        "    - seed\n",
        "\n",
        "<br>\n",
        "\n",
        " 일반적으로 DNN 설계하는 것처럼 각 순서대로 레이어 만들면 된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBtaoRuBVbHX",
        "colab_type": "text"
      },
      "source": [
        "#### DNN 클래스를 위한 클래스, 함수.\n",
        "\n",
        "\n",
        "1. activation_layer : 활성화 함수 반환\n",
        "    - string 형태의 activation 함수 이름 받으면 이름에 해당하는 활성화 레이어 반환.\n",
        "    - activation Layer의 하위 클래스이면 해당하는 레이어 클래스 반환.\n",
        "\n",
        "2. dice: 사용하지 않음.\n",
        "\n",
        "<br> \n",
        " 원래 Dice 클래스 정의해야 하지만, 일단 팀에서는  activation으로 다른 것을 사용하지 않으므로 분석하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSFt0-TLVe6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Activation, Layer\n",
        "\n",
        "\n",
        "def activation_layer(activation):\n",
        "    # if activation in (\"dice\", \"Dice\"):\n",
        "    #     act_layer=Dice()\n",
        "    if isinstance(activation, (str, unicode)):\n",
        "        act_layer = Activation(activation)\n",
        "    elif issubclass(activation, Layer):\n",
        "        act_layer = activation()\n",
        "    else:\n",
        "        raise ValueError(\"activation이 정확하지 않음. 현재: %s. str 형태의 activation layer 이름 혹은 Activation Layer Class를 입력해야 함.\" % (activation))\n",
        "    return act_layer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCqeeX75WTB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Dice(Layer):\n",
        "#     \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
        "#       Input shape\n",
        "#         - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
        "#       Output shape\n",
        "#         - Same shape as the input.\n",
        "#       Arguments\n",
        "#         - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).\n",
        "#         - **epsilon** : Small float added to variance to avoid dividing by zero.\n",
        "#       References\n",
        "#         - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
        "#         self.axis = axis\n",
        "#         self.epsilon = epsilon\n",
        "#         super(Dice, self).__init__(**kwargs)\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         self.bn = tf.keras.layers.BatchNormalization(\n",
        "#             axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "#         self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=Zeros(\n",
        "#         ), dtype=tf.float32, name='dice_alpha')  # name='alpha_'+self.name\n",
        "#         super(Dice, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "#         self.uses_learning_phase = True\n",
        "\n",
        "#     def call(self, inputs, training=None, **kwargs):\n",
        "#         inputs_normed = self.bn(inputs, training=training)\n",
        "#         # tf.layers.batch_normalization(\n",
        "#         # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)\n",
        "#         x_p = tf.sigmoid(inputs_normed)\n",
        "#         return self.alphas * (1.0 - x_p) * inputs + x_p * inputs\n",
        "\n",
        "#     def compute_output_shape(self, input_shape):\n",
        "#         return input_shape\n",
        "\n",
        "#     def get_config(self, ):\n",
        "#         config = {'axis': self.axis, 'epsilon': self.epsilon}\n",
        "#         base_config = super(Dice, self).get_config()\n",
        "#         return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMU0Q-4jS6xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer, BatchNormalization, Dropout\n",
        "from tensorflow.keras.initializers import glorot_normal, Zeros\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "class DNN(Layer):\n",
        "    \n",
        "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, **kwargs):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.l2_reg = l2_reg\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_bn = use_bn\n",
        "        self.seed = seed\n",
        "        super(DNN, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_size = input_shape[-1]\n",
        "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
        "\n",
        "        # weight, bias\n",
        "        self.kernels = [self.add_weight(name='kernel'+str(i),\n",
        "                                        shape=(hidden_units[i], hidden_units[i+1]), # 연결\n",
        "                                        initializer=glorot_normal(seed=self.seed),\n",
        "                                        regularizer=l2(self.l2_reg),\n",
        "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
        "        self.bias = [self.add_weight(name='bias'+str(i),\n",
        "                                     shape=(self.hidden_units[i], ),\n",
        "                                     initializer=Zeros(),\n",
        "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
        "        \n",
        "        # 배치 노멀라이제이션\n",
        "        if self.use_bn:\n",
        "            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]\n",
        "        \n",
        "        # 드롭아웃\n",
        "        self.dropout_layers = [Dropout(self.dropout_rate, seed=self.seed+i) for i in range(len(self.hidden_units))]\n",
        "        \n",
        "        # 활성화\n",
        "        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO_oSE7DW1jb",
        "colab_type": "text"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "> 원래 layers.core에 있음.\n",
        "\n",
        " binary classification, multiclass classification, regression 중 하나의 작업을 수행한다.\n",
        "\n",
        "1. Arguments\n",
        "    - task: binary(binary logloss), regression(regression loss)\n",
        "    - use_bias: bias term 사용 여부\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb9ZBOZ4W3Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.initializers import Zeros\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class PredictionLayer(Layer):\n",
        "\n",
        "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
        "        if task not in ['binary', 'multiclass', 'regression']:\n",
        "            raise ValueError('binary, multiclass, regression 중 하나의 작업을 수행해야 함.')\n",
        "        self.task = task\n",
        "        self.use_bias = use_bias\n",
        "        super(PredictionLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        if self.use_bias:\n",
        "            self.global_bias = self.add_weight(shape=(1, ),\n",
        "                                               initializer=Zeros(),\n",
        "                                               name='global_bias')\n",
        "        super(PredictionLayer, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs \n",
        "        if self.use_bias:\n",
        "            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n",
        "        if self.task == 'binary':\n",
        "            x = tf.sigmoid(x)\n",
        "        \n",
        "        output = tf.reshape(x, (-1, 1))\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1)\n",
        "    \n",
        "    def get_config(self, ):\n",
        "        config = {'task':self.task,\n",
        "                  'use_bias':self.use_bias}\n",
        "        base_config = super(PredictionLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXUbEqLOjg98",
        "colab_type": "text"
      },
      "source": [
        "### Hash\n",
        "\n",
        "> 원래 layers.utils에 있음.\n",
        "\n",
        " util 레이어로, input을 `[0, numbucktes)` 사이의 hash 값으로 바꾼다. mask_zero 옵션을 줄 경우, True, 0, 0.0이 0, 다른 값들은 `[1, num_buckets)` 사이의 값으로 설정된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG1OzuRkjkbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Hash(Layer):\n",
        "    \n",
        "    def __init__(self, num_buckets, mask_zero=False, **kwargs):\n",
        "        self.num_buckets = num_buckets\n",
        "        self.mask_zero = mask_zero\n",
        "        super(Hash, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        super(Hash, self).build(input_shape)\n",
        "    \n",
        "    def call(self, x, mask=None, **kwargs):\n",
        "        if x.dtype != tf.string:\n",
        "            zero = tf.as_string(tf.zeros([1], dtype=x.dtype))\n",
        "            x = tf.as_string(x, )\n",
        "        else:\n",
        "            zero = tf.as_string(tf.zeros([1], dtype='int32'))\n",
        "        \n",
        "        num_buckets = self.num_buckets if not self.mask_zero else self.num_buckets -1\n",
        "\n",
        "        try:\n",
        "            hash_x = tf.string_to_hash_bucket_fast(x, num_buckets, name=None)\n",
        "        except:\n",
        "            hash_x = tf.strings.to_hash_bucket_fast(x, num_buckets, name=None)\n",
        "        \n",
        "        if self.mask_zero:\n",
        "            mask = tf.cast(tf.not_equal(x, zero), dtype='int64')\n",
        "            hash_x = (hash_x + 1)*mask\n",
        "        \n",
        "        return hash_x\n",
        "\n",
        "    def get_config(self, ):\n",
        "        config = {'num_buckets':self.num_buckets,\n",
        "                  'mask_zero':self.mask_zero,}\n",
        "        base_config = self(Hash, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "                "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrgjNjMOQrzJ",
        "colab_type": "text"
      },
      "source": [
        "## **3**. Features\n",
        "\n",
        " 각각의 feature 종류에 해당하는 class들의 모음.\n",
        "\n",
        "* SparseFeat: int형이어야 함.\n",
        "* DenseFeat: float형이어야 함.\n",
        "* VarLenSparseFeat\n",
        "    - ~~프로퍼티 속성 변경해야 하므로 속성 접근 정의해 놓은 듯.~~ 잉 아닌디?\n",
        "    - 정확하게 뭔지 논문 다시 보기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYTddXkENtKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "from tensorflow.keras.initializers import RandomNormal, Zeros\n",
        "\n",
        "\n",
        "class SparseFeat(namedtuple('SparseFeat',\n",
        "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'dtype',\n",
        "                             'embeddings_initializer', 'embedding_name', 'group_name', \n",
        "                             'trainable'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, dtype='int32',\n",
        "                embeddings_initializer=None, embedding_name=None, group_name=DEFAULT_GROUP_NAME,\n",
        "                trainable=False):        \n",
        "\n",
        "        print(\"SparseFeat, __new__ 메소드 호출\")\n",
        "\n",
        "        if embedding_dim == 'auto':\n",
        "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
        "        if embedding_initializer is None:\n",
        "            embeddings_initializer = RandomNormal(mean=0.0, stdev=0.0001, seed=2020)\n",
        "        if embedding_name is None:\n",
        "            embedding_name = name\n",
        "        \n",
        "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, dtype,\n",
        "                                              embeddings_initializer, ebedding_name, group_name,\n",
        "                                              trainable)\n",
        "        \n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()\n",
        "\n",
        "\n",
        "class DenseFeat(namedtuple('DenseFeat',\n",
        "                           ['name', 'dimension', 'dtype'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, name, dimension=1, dtype='float32'):\n",
        "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()\n",
        "\n",
        "\n",
        "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
        "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def __new__(cls, sparsefeat, maxlen, combiner='mean', length_name=None, weight_name=None, weight_norm=True):\n",
        "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name, weight_norm)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.sparsefeat.name\n",
        "\n",
        "    @property\n",
        "    def vocabulary_size(self):\n",
        "        return self.sparsefeat.vocabulary_size\n",
        "    \n",
        "    @property\n",
        "    def embedding_dim(self):\n",
        "        return self.sparsefeat.embedding_dim\n",
        "    \n",
        "    @property\n",
        "    def use_hash(self):\n",
        "        return self.sparsefeat.use_hash\n",
        "    \n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.sparsefeat.dtype\n",
        "    \n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.sparsefeat.dtype\n",
        "    \n",
        "    @property\n",
        "    def embeddings_initializer(self):\n",
        "        return self.sparsefeat.embeddings_initializer\n",
        "    \n",
        "    @property\n",
        "    def embedding_name(self):\n",
        "        return self.sparsefeat.embedding_name\n",
        "    \n",
        "    @property\n",
        "    def group_name(self):\n",
        "        return self.sparsefeat.group_name\n",
        "    \n",
        "    @property\n",
        "    def trainable(self):\n",
        "        return self.sparsefeat.trainable\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return self.name.__hash__()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e7O-RkIQmJT",
        "colab_type": "text"
      },
      "source": [
        "## **4**. Feature로부터 Input 만들기\n",
        "\n",
        "- sparsefeat, densefeat, varlensparsefeat인지에 따라서 input feature 빌딩 방법이 다르다.\n",
        "    - 가변 길이 sparse인 경우, max 길이에 맞춤.\n",
        "    - batch_shape으로 바꿀 수 없는지 확인."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsdmQ4KkP_uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "def build_input_features(feature_columns, prefix=''):\n",
        "    input_features = OrderedDict()\n",
        "    for fc in feature_columns:\n",
        "        if isinstance(fc, SparseFeat):\n",
        "            input_features[fc.name] = Input(shape=(1, ), name=prefix+fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, DenseFeat):\n",
        "            input_features[fc.name] = Input(shape=(fc.dimension, ), name=prefix+fc.name, dtype=fc.dtype)\n",
        "        elif isinstance(fc, VarLenSparseFeat):\n",
        "            input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix+fc.name, dtype=fc.dtype)\n",
        "            if fc.weight_name is not None:\n",
        "                input_features[fc.weight_name] = Input(shape=(fc.maxlen, 1), name=prefix+fc.weight_name, dtype='float32')\n",
        "            if fc.length_name is not None:\n",
        "                input_features[fc.length_name] = Input(shape=(1, ), name=prefix+fc.length_name, dtype='int32')\n",
        "        else:\n",
        "            raise TypeError(\"Feature Column 오류. 현재 Feature Column Type: {}\".shape(type(fc)))\n",
        "    return input_features"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AFLWQXWcoJw",
        "colab_type": "text"
      },
      "source": [
        "### inputs.py\n",
        "\n",
        " 인풋값 만드는 데 필요한 함수들 정의되어 있다.\n",
        "\n",
        "- ~~`create_embedding_dict`~~\n",
        "- ~~`create_embeding_matrix`~~\n",
        "- ~~`embedding_lookup`~~\n",
        "- ~~`get_dense_input`~~\n",
        "- ~~`varlen_embedding_lookup`~~\n",
        "- ~~`get_varlen_pooling_list`~~\n",
        "- ~~`mergeDict`~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IMt-347erk5",
        "colab_type": "text"
      },
      "source": [
        "#### create_embedding_dict\n",
        "\n",
        " sparse feature와 varlen sparse feature에 대해서 임베딩을 만든다. 케라스 임베딩 레이어이고, input_dim, output_dim은 feature에서 애초에 설정되어 있고, trainable 여부는 feature에서 trainable 설정한 여부와 동일하다. 딕셔너리에 임베딩 레이어를 설정해서 저장한다.\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QP595X-eAa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed, l2_reg,\n",
        "                          prefix='sparse_', seq_mask_zero=True):\n",
        "    sparse_embedding = {}\n",
        "    for feat in sparse_feature_columns:\n",
        "        emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
        "                        embeddings_initializer=feat.embeddings_initializer,\n",
        "                        embeddings_regularizer=l2(l2_reg),\n",
        "                        name=prefix+'_emb_'+feat.embedding_name)\n",
        "        emb.trainable = feat.trainable\n",
        "        sparse_embedding[feat.embedding_name] = emb\n",
        "    \n",
        "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
        "        for feat in varlen_sparse_feature_columns:\n",
        "            emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n",
        "                            embeddings_initializer=feat.embeddings_initializer,\n",
        "                            embeddings_regularizer=l2(l2_reg),\n",
        "                            name=prefix+'_seq_emb_'+feat.name,\n",
        "                            mask_zero=seq_mask_zero)\n",
        "            emb.trainable = feat.trainable\n",
        "            sparse_embedding[feat.embedding_name] = emb\n",
        "    return sparse_embedding"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUtCBn8LjIuI",
        "colab_type": "text"
      },
      "source": [
        "#### create_embedding_matrix\n",
        "\n",
        "- `feature_column`에 정의되어 있는 클래스들 활용한다. \n",
        "- 일단 지금은 노트북이므로 import 안 해도 되는 것으로.\n",
        "- 원래는 아래처럼 써야 한다.\n",
        "```\n",
        "from . import feature_column as fc_lib\n",
        "\n",
        "    sparse_feature_columns = list(\n",
        "    filter(lambda x: isinstance(x, fc_lib.SparseFeat), feature_columns)) if feature_columns else []\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su55ThPwcrYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True):\n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
        "    sparse_emb_dict = create_embedding_dict(sparse_features, varlen_sparse_feature_columns, seed,\n",
        "                                            l2_reg, prefix=prefix + 'sparse', seq_mask_zero=seq_mask_zero)\n",
        "    return sparse_emb_dict"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YRntXyLhUT1",
        "colab_type": "text"
      },
      "source": [
        "#### embedding_lookup / varlen_embedding_lookup\n",
        "\n",
        " hash 혹은 feauture name 사용해서 embedding을 찾는다.\n",
        "\n",
        " - embedding_lookup은 defaultdict 해서 list flatten하고."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph0U5KKOgie4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "\n",
        "\n",
        "def embedding_lookup(spasre_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(), \n",
        "                     mask_feat_list=(), to_list=False):\n",
        "    group_embedding_dict=defaultdict(list)\n",
        "    for fc in sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        embedding_name = fc.embedding_name\n",
        "        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n",
        "            if fc.use_hash:\n",
        "                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list))(\n",
        "                    sparse_input_dict[feature_name])\n",
        "            else:\n",
        "                lookup_idx = sparse_input_dict[feature_name]\n",
        "            \n",
        "            group_emgedding_dict[fc.group_name].append(spasre_embedding_dict[embedding_name](lookup_idx))\n",
        "        if to_list:\n",
        "            return list(chain.from_iterable(group_embedding_dict.value()))\n",
        "        return group_embedding_dict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN6p8XUqgiWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
        "    varlen_embedding_vec_dict = {}\n",
        "    for fc in varlen_sparse_feature_columns:\n",
        "        feature_name = fc.name\n",
        "        embedding_name = fc.embedding_name\n",
        "        if fc.use_hash:\n",
        "            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True)(sequence_input_dict[feature_name])\n",
        "        else:\n",
        "            lookup_idx = sequence_input_dict[feature_name]\n",
        "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
        "    return varlen_embedding_vec_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz_-y-KHjBSo",
        "colab_type": "text"
      },
      "source": [
        "#### get_dense_input\n",
        "\n",
        "\n",
        "- `feature_column`에 정의되어 있는 클래스들 활용한다. \n",
        "- 일단 지금은 노트북이므로 import 안 해도 되는 것으로.\n",
        "- 원래는 아래처럼 써야 한다.\n",
        "```\n",
        "from . import feature_column as fc_lib\n",
        "\n",
        "    sparse_feature_columns = list(\n",
        "    filter(lambda x: isinstance(x, fc_lib.DenseFeat), feature_columns)) if feature_columns else []\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpT5AymVi8OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dense_input(features, feature_columns):\n",
        "    dense_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if feature_columns else []\n",
        "    dense_input_list = [features[fc.name] for fc in dense_feature_columns]\n",
        "    return dense_input_list"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9HymcY3jscL",
        "colab_type": "text"
      },
      "source": [
        "#### get_varlen_pooling_list\n",
        "\n",
        " 가변 길이 sequence 풀링해서 리스트를 반환한다.\n",
        "\n",
        " - weight_name 있으면 WeightedSequenceLayer 사용하고,\n",
        " - weight_name 없으면 weight 안 주는 거로 생각해서 embedding 사용하는데,\n",
        "<br>\n",
        "\n",
        " 그렇게 해서 SequencePoolingLayer로 간다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK7OqzuzjxaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_columns, to_list=False):\n",
        "    pooling_vec_list = defaultdict(list)\n",
        "    for fc in varlen_sparse_columns:\n",
        "        feature_name = fc.name\n",
        "        combiner = fc.combiner\n",
        "        feature_length_name = fc.feature_length_name\n",
        "        if feature_length_name is not None:\n",
        "            if fc.weight_name is not None:\n",
        "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n",
        "                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n",
        "            else:\n",
        "                seq_input = embedding_dict[feature_name]\n",
        "            vec = SeuqencePoolingLayer(combinder, supports_masking=False)(\n",
        "                [seq_input, features[feature_length_name]])\n",
        "        else:\n",
        "            if fc.weight_name is not None:\n",
        "                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(\n",
        "                    [embedding_dict[feature_name], features[fc.weight_name]])\n",
        "            else:\n",
        "                seq_input = embedding_dict[feature_name]\n",
        "            vec = SequencePoolingLayer(combinder, supports_masking=True)(seq_input)\n",
        "        \n",
        "        pooling_vec_list[fc.group_name].append(vec)\n",
        "        \n",
        "    if to_list:\n",
        "        return chain.from_iterable(pooling_vec_list.values())\n",
        "    return pooling_vec_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IDWzczwEiq",
        "colab_type": "text"
      },
      "source": [
        "#### mergeDict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxbMOTQ9wFxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def mergeDict(a, b):\n",
        "    c = defaultdict(list)\n",
        "    for k, v in a.items():\n",
        "        c[k].extend(v)\n",
        "    for k, v in b.items():\n",
        "        c[k].extend(v)\n",
        "    return c"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLnPJac5c6MP",
        "colab_type": "text"
      },
      "source": [
        "### input_from_feature_columns\n",
        "\n",
        " 다시 input 만들기로 돌아 와서.\n",
        "\n",
        "\n",
        " `inputs.py`에 정의되어 있는 함수들을 이용해서 input 값을 만든다.\n",
        "\n",
        "\n",
        "\n",
        "- filter를 통해 각 column이 어디에 속하는지 확인한 후 list로 반환. 단, feature_columns 인자 없으면 빈 리스트.\n",
        "- embedding matrix를 `create_embedding_matrix` 함수를 가지고 만든 후,\n",
        "- embedding_lookup을 통해 위에서 만든 dictionary에서 feature의 embedding을 찾는다. 그런데 이건 sparse feature에 대해서만.\n",
        "- dense feature에 대해서는 get_dense_input한 뒤, 위와 비슷한 방식으로 함수들을 이용해서 embedding matrix 만들고 찾고.\n",
        "- 만약에 dense support하지 않는데 dense column 있다면 잘못한 것이므로 오류.\n",
        "- 마지막에 mergeDict로 다 합친다. 그런데 그룹핑하지 않으려면 chain해서 flatten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlQ3CmyDa35F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n",
        "                               support_dense=True, support_group=False):\n",
        "    \n",
        "    sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
        "    varlen_sparse_feature_columns = list(\n",
        "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
        "    \n",
        "    embedding_matrix_dict = create_embedding_matrix(feature_colums, l2_reg, seed, prefix=prefix, \n",
        "                                                    seq_mask_zero=seq_mask_zero)\n",
        "\n",
        "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
        "    dense_value_list = get_dense_input(features, feature_columns)\n",
        "\n",
        "    if not support_dense and len(dense_value_list) > 0:\n",
        "        raise ValueError(\"DenseFeature DNN에서 사용할 수 없음.\")\n",
        "    \n",
        "    seq_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
        "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
        "                                                                 varlen_sparse_feature_columns)\n",
        "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
        "    if not support_group:\n",
        "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
        "    return group_embedding_dict, dense_value_list"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1OgW9_jYs3V",
        "colab_type": "text"
      },
      "source": [
        "## **5**. 로짓 계산 \n",
        "\n",
        " linear 로짓, fm 로짓을 계산해야 하는데, fm 로짓은 나중에 모델 자체에서 계산한다. linear 로짓은 함수를 만들어서 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TMU7uXyhU-U",
        "colab_type": "text"
      },
      "source": [
        "### get_linear_logit\n",
        "\n",
        " linear 로짓을 계산한다.\n",
        "\n",
        "- deepcopy 안 해도 되는지?\n",
        "- `_replace`로 namedtuple 프로퍼티 변경\n",
        "- 프로퍼티 바꾸고 할당해야 하므로 enumerate 안 될 듯."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlphNjGYvh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_linear_logit(features, feature_columns, units=1, use_bias=False, seed=1024, prefix='linear', l2_reg=0):\n",
        "    linear_feature_columns = copy(feature_columns)\n",
        "    for i in range(len(linear_feature_columns)):\n",
        "        if isinstance(linear_feature_columns[i], SparseFeat):\n",
        "            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1, \n",
        "                                                                           embeddings_initializer=Zeros())\n",
        "        if isinstance(linear_feature_columns[i], VarLenSparseFeat):\n",
        "            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1,\n",
        "                                                                           embedidngs_initializer=Zeros())\n",
        "    \n",
        "    linear_emb_list = [input_from_feature_columns(features, linear_feature_columns, l2_reg, seed,\n",
        "                                                  prefix=prefix + str(i))[0] for i in range(units)]\n",
        "    _, dense_input_list = input_from_feature_columns(features, linear_feature_columns, l2_reg, seed, prefix=prefix)\n",
        "\n",
        "    linear_logit_list = []\n",
        "    for i in range(units):\n",
        "        if len(linear_emb_list[i])>0 and len(dense_input_list)>0:\n",
        "            sparse_input = concat_func(linear_emb_list[i])\n",
        "            dense_input = concat_func(dense_input_list)\n",
        "            linear_logit = Linear(l2_reg, mode=2, use_bias=use_bias, seed=seed)([sparse_input, dense_input])\n",
        "        elif len(linear_emb_list[i]) > 0:\n",
        "            sparse_input = concat_func(linear_emb_list[i])\n",
        "            linear_logit = Linear(l2_reg, mode=0, use_bias=use_bias, seed=seed)(sparse_input)\n",
        "        elif len(dense_input_list) > 0:\n",
        "            dense_input = concat_func(dense_input_list)\n",
        "            linear_logit = Linear(l2_reg, mode=1, use_bias=use_bias, seed=seed)(dense_input)\n",
        "        else:\n",
        "            return add_func([])\n",
        "        linear_logit_list.append(linear_logit)\n",
        "\n",
        "    return concat_func(linear_logit_list)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3R-F3qnWKP-",
        "colab_type": "text"
      },
      "source": [
        "## **6** DeepFM\n",
        "\n",
        " FM 모델을 deep 네트워크로 구현한다.\n",
        "\n",
        " \n",
        "1. 파라미터\n",
        "    - `linear_feature_columns`: linear part에 사용될 feature들의 이터러블.\n",
        "    - `dnn_feature_columns`: deep part에 사용될 feature들의 이터러블.\n",
        "    - `fm_group`: feature 상호작용에 사용될 feature들의 리스트와 그것의 그룹 이름(지정하면 되나?)\n",
        "    - `dnn_hidden_units`: DNN 각각의 레이어에 사용될 layer number, units의 리스트(각각은 양수이거나 빈 리스트여야 함).\n",
        "    - `l2_reg_linear`: 실수. linear part에 적용될 L2 규제항.\n",
        "    - `l2_reg_embeddng`: 실수. embedding vector에 적용할 L2 규제항.\n",
        "    - `l2_reg_dnn`: 실수. DNN에 적용할 L2 규제항.\n",
        "    - `seed`: 시드값.\n",
        "    - `dnn_dropout`: DNN 네트워크 노드 드롭아웃 비율.\n",
        "    - `dnn_activation`: DNN 활성화 함수.\n",
        "    - `dnn_use_bn`: DNN에서 배치 노멀라이제이션 여부.\n",
        "    - `task`: binary(binary logloss), regression(regression loss)\n",
        "\n",
        "2. 리턴 : 케라스 모델 객체."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2masV2XXuZq",
        "colab_type": "text"
      },
      "source": [
        "* linear column, dnn column 리스트로 받아서, input feature 빌드한다.\n",
        "* ordered dictionary 반환되는데, 거기서 values만 받아 리스트로 만든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNseMZoOYkeg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQjz4opXQUGU",
        "colab_type": "text"
      },
      "source": [
        " linear logit을 계산하고, fm logit(factorization을 통한 상호작용)을 계산한다. \n",
        "\n",
        "* `build_input_features`를 통해 입력 피쳐를 빌드한다. linear_features와 dnn_features를 나누어 build_input_features 함수에 인자로 넘긴다. 그리고 linear_logit을 계산한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3TjFEr_Mxwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "def DeepFM(linear_feature_columns, dnn_feature_columns, fm_group=[DEFAULT_GROUP_NAME], dnn_hidden_units=(128, 128),\n",
        "           l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,\n",
        "           dnn_activation='relu', dnn_use_bn=False, task='binary'):\n",
        "    features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
        "    inputs_list = list(features.values())\n",
        "    \n",
        "    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',\n",
        "                                    l2_reg=l2_reg_linear)\n",
        "    \n",
        "    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,\n",
        "                                                                        seed, support_group=True)\n",
        "    \n",
        "    fm_logit = add_func([FM()(concat_func(v, axis=1))\n",
        "                        for k, v in group_embedding_dict.items() if k in fm_group])\n",
        "    \n",
        "    dnn_input = combined_dnn_input(list(chain.from_iterable(group_embedding_dict.values())),\n",
        "                                   dense_value_list)\n",
        "    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
        "                     dnn_use_bn, seed)(dnn_input)\n",
        "    dnn_logit = Dense(1, use_bias=False, kernel_initializer=glorot_normal(seed=seed))(dnn_output)\n",
        "    \n",
        "    final_logit = add_func([linear_logit, fm_logit, dnn_logit])\n",
        "\n",
        "    output = PredictionLayer(task)(final_logit)\n",
        "    model = Model(inputs=inputs_list, outputs=output)\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}